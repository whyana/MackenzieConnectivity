---
title: "1_MackenzieDeltaConnectivityAnalysis_classGeneration"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Libraries
```{r}
library(tidyverse)
library(sf)
library(lubridate)
library(grDevices)
library(mapview)
library(extrafont)
library(ggpubr)
library(ggmap)
library(RgoogleMaps)
library(broom)
#library(HistDAWass)
library(tidyhydat)
library(sp)
library(data.table)
library(ggalluvial)
#library(gridExtra)
#library(grid)
#library(cowplot)
library(patchwork)
library(magick)
library(units)
library(Kendall)
library(ggspatial)
library(dtplyr)
#Import libraries for Random Forest
library(caret) 
library(e1071)
library(Boruta)
library(tidymodels)
library(skimr)
library(vip)
```

# Functions
```{r}
#takes RGB to calculate dominant wavelength
chroma <- function(R, G, B) {  
  require(colorscience)

# Convert R,G, and B spectral reflectance to dominant wavelength #based
# on CIE chromaticity color space

# see Wang et al 2015. MODIS-Based Radiometric Color Extraction and
# Classification of Inland Water With the Forel-Ule
# Scale: A Case Study of Lake Taihu

# chromaticity.diagram.color.fill()
Xi <- 2.7689*R + 1.7517*G + 1.1302*B
Yi <- 1.0000*R + 4.5907*G + 0.0601*B
Zi <- 0.0565*G + 5.5943*B

# calculate coordinates on chromaticity diagram
x <-  Xi / (Xi + Yi +  Zi)
y <-  Yi / (Xi + Yi +  Zi)
z <-  Zi / (Xi + Yi +  Zi)

# calculate hue angle
alpha <- atan2( (x - (1/3)), (y - (1/3))) * 180/pi

# make look up table for hue angle to wavelength conversion
cie <- cccie31 %>%
  dplyr::mutate(a = atan2( (x - (1/3)), (y - (1/3))) * 180/pi) %>%
  dplyr::filter(wlnm <= 700) %>%
  dplyr::filter(wlnm >=380) 

# find nearest dominant wavelength to hue angle
wl <- cie[as.vector(sapply(alpha,function(x) which.min(abs(x - cie$a)))) , 'wlnm']

return(wl)
}
```

# Imports and file paths
```{r}
# dates for version control
stage1.date = "20230308" # the first data join phase
stage2.date = "20230309" # The classified training testing data via random forest
stage3.date = "20230309" # Classification of all mackenzie delta lakes
stage4.date = "20230309" # date of export of gif

# Names of files and folders for reflectance data
import.filePath = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/GEE Downloads"
import.Lakes = "MackenzieLakeExport_20230307_allYears.csv"
import.channels = "MackenzieChannelExport_20230307_allYears.csv"
import.sword = "na_sword_reaches_hb82_v14.shp"

# intermediate working directory
int.wd="C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/intermediaryDownloads"


#Name of file and folder for lake shapefiles & island polygon shapefiles
shapeFiles.filePath = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/shapeFiles"
lakes.shapeFile = "mackenzieGoodLakes.shp"
islands.shapeFile = "vectorIslandArea2.shp"
setwd(shapeFiles.filePath)
lakes.sf = st_read(lakes.shapeFile)
islands.sf=st_read(islands.shapeFile)
gif.wd = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/images/GIF_20230303"
images.wd = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/images"
pil.wd = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/PiliourasAndRowland/"
# 
# # Name of file and folder for GECI validation data
valFile.path="C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/GEE Downloads/trainingData"
valFileName = "trainingData_1819lakes.shp"
# 
# # Name of file and folder for figures
# figures.filePath = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Papers/ColvilleDeltaConnectivity/Figures/Figures_raw"
# dens.figure.name = "densityPlotExample.pdf"
# tree.figure.name = "DecisionTreeFigure.pdf"

```


# Import and filter the lake and channel data  - calculates both for mean, tenth percentile and 90% dom wv values.
```{r}
setwd(import.filePath)
#import lake data
all.lakes = read.csv(import.Lakes) %>% dplyr::as_tibble()
#import all lake buffer/channel file
all.channels = read.csv(import.channels) %>% dplyr::as_tibble()


# Filter lake and channel data
lakes.maxPix = all.lakes %>% group_by(OBJECTID) %>% summarise(max.pix = max(Red_count))# count max number of pix--> likely the clear sky total of the lake
lakes.filter = all.lakes %>% 
  left_join(lakes.maxPix, by="OBJECTID") %>% 
  mutate(dateTime = as_datetime(system.time_start_mean*0.001),
         date = as_date(dateTime)) %>% 
  dplyr::select(-system.time_start_mean, -count) %>% 
  dplyr::filter(Red_count/max.pix >= 0.5)  # each observation must be made up of at least 50% of the lake area
lakes.combo=lakes.filter
channels.maxPix = all.channels %>% group_by(fid) %>% summarise(max.pix = max(Red_count))
channels.filter = all.channels %>% 
  left_join(channels.maxPix, by="fid") %>% 
  mutate(dateTime = as_datetime(`system.time_start_mean`*0.001),
         date=as_date(dateTime)) %>% 
  dplyr::select(-`system.time_start_mean`) %>% as_tibble() %>% 
  dplyr::filter(Red_count/max.pix >=0.5) 
  #lastly, remove coastal islands by fid.
  
rm(lakes.filter, all.lakes, all.channels)
gc()

channels.combo = channels.filter%>% as_tibble() %>% 
  rename(Blue_chan_m = Blue_mean,Blue_chan_p10 = Blue_p10,Blue_chan_p90 = Blue_p90 ,
                 Gb_chan_m = Gb_ratio_mean, Gb_chan_p10 = Gb_ratio_p10,Gb_chan_p90 = Gb_ratio_p90,
                 Green_chan_m=Green_mean,Green_chan_p10=Green_p10,Green_chan_p90=Green_p90, 
                 Ndssi_chan_m=Ndssi_mean,Ndssi_chan_p10=Ndssi_p10, Ndssi_chan_p90=Ndssi_p90, 
                 Nsmi_chan_m=Nsmi_mean,Nsmi_chan_p10=Nsmi_p10, Nsmi_chan_p90=Nsmi_p90,
                 Nir_chan_m = Nir_mean, Nir_chan_p10 = Nir_p10, Nir_chan_p90=Nir_p90,
                 Ndti_chan_m = Ndti_mean, Ndti_chan_p10 = Ndti_p10, Ndti_chan_p90=Ndti_p90,
                 Nsmi_mod_chan_m = Nsmi_mod_mean, 
         Nsmi_mod_chan_p10=Nsmi_mod_p10, Nsmi_mod_chan_p90=Nsmi_mod_p90,
                 Red_chan_m=Red_mean, Red_chan_p10=Red_p10, Red_chan_p90=Red_p90, 
         count_chan= Red_count,
         NirSwir1_chan_m= NirSwir1_mean, NirSwir1_chan_p10= NirSwir1_p10,
         NirSwir1_chan_p90= NirSwir1_p90, NirGreen_chan_m= NirGreen_mean,
         NirGreen_chan_p10= NirGreen_p10,NirGreen_chan_p90= NirGreen_p90,
         BlueNir_chan_m= BlueNir_mean,BlueNir_chan_p10= BlueNir_p10,
         BlueNir_chan_p90= BlueNir_p90,
         NirRedBlue_chan_m= NirRedBlue_mean, NirRedBlue_chan_p10= NirRedBlue_p10,
         NirRedBlue_chan_p90= NirRedBlue_p90) 
rm(channels.filter)
gc()

# Join lake and channel data from the same date and calculate the dominant wavelength ratio
lakes.combo=data.table(lakes.combo) %>% 
  group_by(OBJECTID, date) %>%  #only need to do if we have more than 1 obs per day (pick one with most pixels)
  mutate(my_ranks = order(Red_count, decreasing=TRUE)) %>% 
  filter(my_ranks==1) 
lakes.combo=data.table(lakes.combo)
channels.combo=data.table(channels.combo) %>% group_by(fid, date) %>%  #only need to do if we have more than 1 obs per day (pick one with most pixels)
  mutate(my_ranks = order(count_chan, decreasing=TRUE)) %>% 
  filter(my_ranks==1)
channels.combo=data.table(channels.combo)
gc()
# Figure out which lakes go with which island
lake.island.key = lakes.sf %>% st_transform(islands.sf %>% st_crs()) %>% st_join(islands.sf, left=T) %>% dplyr::filter(!is.na(fid)) %>% dplyr::select(fid, OBJECTID)


# Perfom the join
joined.df = lakes.combo %>% left_join(lake.island.key, by="OBJECTID") %>% dplyr::select(-dateTime, -my_ranks, -geometry, -area_m) %>% 
  left_join(channels.combo, by=c("fid", "date", "constant_mean")) %>% filter(!is.na(Red_chan_m)) #filter out data with NA reflectance values

# Calculate dominate wavelength ratio values for lakes and channels - NO LONGER NEED TO DIVIDE
## lakes
m_lake_r = joined.df$Red_mean/1000
m_lake_g = joined.df$Green_mean/1000
m_lake_b = joined.df$Blue_mean/1000
dw_lake = chroma(m_lake_r, m_lake_g, m_lake_b)
## Channels
m_chan_r = joined.df$Red_chan_m/1000
m_chan_g = joined.df$Green_chan_m/1000
m_chan_b = joined.df$Blue_chan_m/1000
dw_chan = chroma(m_chan_r, m_chan_g, m_chan_b)
joined.df$dom_wv_lake_m = dw_lake
joined.df$dom_wv_chan_m = dw_chan

# calculate ratios
channels.lakes = joined.df %>% 
  mutate(dom_wv_ratio_m = dom_wv_lake_m/dom_wv_chan_m,
         G_ratio_m = Green_mean/Green_chan_m, 
         B_ratio_m = Blue_mean/Blue_chan_m,
         R_ratio_m = Red_mean/Red_chan_m,
         Nir_ratio_m = Nir_mean/Nir_chan_m,
         Gb_ratio_m = Gb_ratio_mean/Gb_chan_m,
         Ndssi_ratio_m = Ndssi_mean/Ndssi_chan_m,
         Nsmi_ratio_m =Nsmi_mean/Nsmi_chan_m,
         Nsmi_mod_ratio_m = Nsmi_mod_mean/Nsmi_mod_chan_m,
         Ndti_ratio_m = Ndti_mean/Ndti_chan_m,
         NirSwir1_ratio_m = NirSwir1_mean/NirSwir1_chan_m,
         NirGreen_ratio_m = NirGreen_mean/NirGreen_chan_m,
         BlueNir_ratio_m = BlueNir_mean/BlueNir_chan_m,
         NirRedBlue_ratio_m = NirRedBlue_mean/NirRedBlue_chan_m) %>% 
  dplyr::filter(!is.na(Nsmi_ratio_m)) %>%  #Doesn't matter which joined variable you use here--just filtering those that don't have a match
  dplyr::select(-dateTime, -delta, -my_ranks, -max.pix.y, -max.pix.x) %>% 
  mutate(year=year(date))
gc()

setwd(int.wd)
write_rds(channels.lakes, paste0("joinedData_", stage1.date,".Rdata"))
channels.lakes = read_rds(paste0("joinedData_", stage1.date,".Rdata"))
write_rds(channels.combo, paste0("channelData_", stage1.date,".Rdata"))
write_rds(lakes.combo, paste0("lakeData_", stage1.date,".Rdata"))


```



# Prep data for  testing and training
```{r}
# Import the joined lake/channel data
setwd(int.wd)
channels.lakes=read_rds(paste0("joinedData_", stage1.date,".Rdata")) %>% lazy_dt()

# For each lake in  the validation period, calculate the median, sdev, and kurtosis for each possible band combination. Filter to just the data in the validation periods (2020)

prep.2020= channels.lakes%>%filter(year==2020) %>% 
  mutate(group = case_when(
    (yday(date)>=162 &yday(date)<=182) ~ 
      "high discharge", #training imagery from high discharge period in 2020
    (yday(date)>235 & yday(date)<=254)~
      "low discharge" # training imagery from low discharge period in 2020
  )) %>% drop_na() %>% 
  dplyr::group_by(OBJECTID, year, group) %>%
  dplyr::summarise(med_dw_m = median(dom_wv_lake_m),
           med_R_m = median(Red_mean),
           med_B_m = median(Blue_mean),
           med_G_m = median(Green_mean),
           med_Nir_m = median(Nir_mean),
           med_Gb_m = median(Gb_ratio_mean),
           med_Ndssi_m = median(Ndssi_mean),
           med_Nsmi_m = median(Nsmi_mean),
           med_Nsmi_mod_m = median(Nsmi_mod_mean),
           med_Ndti_m = median(Ndti_mean),
           med_NirSwir1_m = median(NirSwir1_mean),
           med_NirGreen_m = median(NirGreen_mean),
           med_BlueNir_m = median(BlueNir_mean),
          med_NirRedBlue_m = median(NirRedBlue_mean),
           count=n()
           ) %>% ungroup() %>% 
  dplyr::filter(count>=2) %>% as_tibble()# require at least 2 or more observations in each period

```

# Supervized classification - Random Forest & Training/Testing evaluation
```{r}
# Import training/testing data into the script, break our much larger classification scheme into 3 classes
setwd(valFile.path)
lake.class = st_read(valFileName , stringsAsFactors = F) %>% as_tibble() %>% 
  rename(OBJECTID=ID) %>% 
  group_by(OBJECTID) %>% mutate(row.num=row_number()) %>% filter(row.num==1) %>% 
  dplyr::select(-row.num) %>% 
  ungroup() %>% 
  mutate(high.dis.class = case_when(
   type == "g1" | type== "g7" | type== "g4" | type=="lowThenMedium" ~ 0,
   type== "g3_5" | type=="mediumThenHigh" | type == "moderateBoth"~ 1,
   type == "g2_5" | type == "g3"  | type =="g6" | type == "g2" | type=="g5" |type=="g2_add" ~ 2
  ),
  low.dis.class = case_when(
    type == "g1" | type== "g7" | type== "g4" | type== "g3_5" | type == "g3"  | type =="g6"~ 0 ,
    type == "g2_5" | type == "moderateBoth" | type=="lowThenMedium" ~ 1,
    type == "g2" | type=="g5" |type=="g2_add" | type=="mediumThenHigh" ~ 2
  )
  ) %>% 
  gather(Key, classes, -OBJECTID, -type, -geometry) %>% 
  mutate(group = ifelse(Key=="high.dis.class", "high discharge", "low discharge")) %>% 
  dplyr::select(-Key, -type, -geometry) %>% 
  dplyr::filter(!is.na(classes) )  # Remove NA class values -- includes lakes whose class was obscured by clouds or bad imagery

# Join the remote sensing data to the lake classificatin data

trainData <- prep.2020 %>%   
  left_join(lake.class, by=c("OBJECTID", "group")) %>% 
  dplyr::select(-OBJECTID, -year, -count, -group) %>% drop_na() 


### Plot the colors of all the lakes
# devtools::install_github("roaldarbol/wavecolour")
# library(wavecolour) # I needed to restart R for this and didn't want to


  
  
# Investigate reflectance variables that are correlated to each other, remove high correlations
## Plot all correlations
corrplot::corrplot(trainData  %>% as.matrix() %>% cor(), order="hclust", method="shade", diag=F)
## Remove correlations
# Boruta analysis to identify important variables 
## remove unnecessary columns using Boruta
attach(trainData)
colnames(trainData)
set.seed(300)
## apply the boruta
boruta.train <- Boruta(classes~., data = trainData,
                       mcAdj = TRUE, pValue = 0.01,doTrace = 2,ntree = 50)
## print the results of the boruta
print(boruta.train)
boruta.train$finalDecision
## plot the results of the boruta
plot(boruta.train, colCode=c("lightgreen", "yellow", "red", "blue"),
     cex.axis=.7, las=2, xlab="",sort=TRUE, main="") 
## Get rid of tentative (yellow on the plot)
boruta.bank=TentativeRoughFix(boruta.train)
plot(boruta.bank, colCode=c("lightgreen", "yellow", "red", "blue"),
     cex.axis=.7, las=2, xlab="",sort=TRUE, main="") 

##Select the variables to use during the classification
important.cols=getSelectedAttributes(boruta.bank, withTentative = F) 
bank_df <- attStats(boruta.bank)
print(bank_df) #visualize variable importance  
bank_df$variable = rownames(bank_df)
bank_df=bank_df %>% arrange(medianImp)
bank_df$variable = factor(bank_df$variable, levels=bank_df$variable)

# keep R, and then remove things that are highly correlated to hit (pos or neg)
cor.matrix = cor(trainData %>% select(-classes), method="spearman")
rownm = rownames(cor.matrix)
cor.tibble = cor.matrix %>% as_tibble()
cor.tibble$variable = rownm
remove.variables = cor.tibble %>% filter(abs(med_R_m) >=0.8) %>% filter(variable!="med_R_m")%>% select(med_R_m, variable)
remove.variables = remove.variables$variable
remove.variables

`%ni%` <- Negate(`%in%`)
filtered.trainData = subset(trainData, select = names(trainData) %ni% remove.variables)

# look through the remaining variables and make sure there aren't any other correlations. If there are, remove them.
filt.cor = cor(filtered.trainData %>% select(-classes), method="spearman")
filt.index = caret::findCorrelation(filt.cor, cutoff=0.8)

keep.names = colnames(filtered.trainData[,-filt.index] %>% select(-classes)) # these are the column names to keep




# Prep data for random forest classification, 
set.seed(123)
## Join the reflectance and lake classification data,keep only important less correlated columns
geci.val =  prep.2020 %>% 
  left_join(lake.class, by=c("OBJECTID", "group")) %>% 
  dplyr::select(c("OBJECTID","classes", all_of(keep.names))) %>% drop_na() %>% 
  mutate(classes=as.factor(classes))
## look at the data to make sure it is as expected
skimr::skim(geci.val)
## split data into test and train
set.seed(234)
geci.split = initial_split(geci.val, strata=classes)
geci.train = training(geci.split)
geci.test = testing(geci.split)

# Start random forest classification
geci.rec =recipe(classes ~., data=geci.train) %>% 
  update_role(OBJECTID, new_role = "OBJECTID")# %>% 
 # step_corr(all_numeric_predictors(), threshold = .8)

geci.pre=prep(geci.rec)
geci.juiced = juice(geci.pre)
geci.juiced %>% count(classes)
## Make model specifications & get ready to tune
tune.spec = rand_forest(
  mtry=tune(), #when you are making leaves of the tree, how many do you sample at each split--all predictors or just a few
  trees = 500,
  min_n=tune()# How long do you keep splitting. How many data points have to be in a node before you stop splitting
  ) %>% 
  set_mode("classification") %>% set_engine("ranger") #ranger is just one way of doing random forest
tune.wf=workflow() %>% 
  add_recipe(geci.rec) %>% 
  add_model(tune.spec)
## Train hyperparameters with 5-fold cross validation
set.seed(345)
geci.fold  = vfold_cv(geci.train, v=5, strata=classes)
## tune parameters
tune.res=tune_grid(
  tune.wf,
  resamples=geci.fold,
  grid=20
)
## take a look at parameters
tune.res %>% collect_metrics() #look at all the metrics
tune.res %>% select_best("roc_auc") #select best accuracy
tune.res %>% 
  collect_metrics() %>% 
  dplyr::filter(.metric=="roc_auc") %>% 
  dplyr::select(mean, min_n, mtry) %>% 
  pivot_longer(min_n:mtry,
               values_to="value",
               names_to="parameter") %>% 
  ggplot(aes(value, mean, color=parameter))+
  geom_point(show.legend="FALSE")+
  facet_wrap(~parameter, scales="free_x")+theme_bw()+
  theme(axis.text = element_text(size=12),
        axis.title = element_text(size=14, face="bold"),
        strip.text = element_text(size=14, face="bold"))+ylab("mean ROC AUC ")

set.seed(456)

## select the best option based on the ROC_AUC parameter
best.acc =select_best(tune.res, "roc_auc") 
final.rf=finalize_model(
  tune.spec,
  best.acc
)
## Check out variable importance for the model as a whole
final.rf %>% set_engine("ranger", importance="permutation") %>% 
  fit(classes~.,
      data = juice(geci.pre) %>% dplyr::select(-OBJECTID)) %>% 
  vip(geom="point")+theme_bw()
# see how the model does on the testing data
final.wf = workflow() %>% 
  add_recipe(geci.rec) %>% 
  add_model(final.rf)
final.res=final.wf %>% last_fit(geci.split)
final.res %>% collect_metrics()
final.res %>% collect_predictions() 

# Select the final model & apply it to the training/testing dataset
fitted.wf.rf= pluck(final.res, 6)[[1]]
train.ids = geci.train$OBJECTID
test.ids = geci.test$OBJECTID
final.pred.cm = cbind(predict(fitted.wf.rf, geci.val), geci.val) %>% as_tibble() %>% 
  dplyr::select(OBJECTID, .pred_class, classes)%>% rename(.obs_class = classes) %>% 
  mutate(split=case_when(
    OBJECTID %in% train.ids~"train", 
    OBJECTID %in% test.ids~"test"))

## Save the training/test classifications
setwd(int.wd)
write_rds(final.pred.cm, paste0("predictions_traintest_",stage2.date, ".Rdata"))
final.pred.cm=read_rds(paste0("predictions_traintest_",stage2.date, ".Rdata"))
## Plot a confusion matrix
### Just testing split
table.test=confusionMatrix(final.pred.cm[final.pred.cm$split=="test",]$.pred_class,
                      final.pred.cm[final.pred.cm$split=="test",]$.obs_class)
### All training and testing
table.all=confusionMatrix(final.pred.cm$.pred_class,
                      final.pred.cm$.obs_class)

table.test
table.all

# Calculate Gwets AC1 (alternative to kappa statistic) https://cran.r-project.org/web/packages/irrCAC/vignettes/weighting.html
#relevant paper https://support.sas.com/resources/papers/proceedings/proceedings/forum2007/186-2007.pdf
#install.packages("irrCAC")
library(irrCAC)

test.cm  = table.test$table
q = nrow(test.cm)
gwet.ac1.table(test.cm)
gwet.ac1.table(test.cm, weights = radical.weights(1:q))
gwet.ac1.table(test.cm, weights = linear.weights(1:q))# --- use this one I think?
gwet.ac1.table(test.cm, weights = quadratic.weights(1:q))

# Plot ggalluvial plot showing classificatâ€)ion for the testing data (actual vs predicted)
## Prep the data
alluvial.prep= cbind(predict(fitted.wf.rf, geci.val), geci.val) %>% as_tibble() %>%  # apply the model
  dplyr::select(OBJECTID, .pred_class, classes)%>% rename(.obs_class = classes) %>%  # shape the data
  mutate(split=case_when(
    OBJECTID %in% train.ids~"train", 
    OBJECTID %in% test.ids~"test")) %>% # group the data into training vs testing
  dplyr::filter(split=="test") %>%  # select only test data
  group_by(.pred_class, .obs_class) %>% summarise(freq=n()) 
## Create the plot
library(ggalluvial)
ggplot(data=alluvial.prep, aes(axis1=.obs_class, axis2 = .pred_class, y=freq))+
  geom_alluvium(aes(fill=.pred_class), width=0)+
  geom_stratum(aes(fill=.pred_class), width=0.15, color="white")+
  #geom_text(stat="stratum", size=4)+
  scale_fill_manual(values=c("#619CFF", "#00BA38", "#F8766D"))+
  geom_label(stat = "stratum",
            aes(label = after_stat(stratum)), size=12) +
  scale_x_discrete(limits = c("Survey", "Response"),
                   expand = c(0.15, 0.05)) +
  theme_void()+theme(legend.title=element_blank(), legend.position = "none")

setwd(images.wd)
ggsave("alluvial.png", device="png", width=8, height = 6.5, units="in")

```


# Apply random forest model to all lakes in all landsat images
```{r}
# import the data and get it prepped for the classification
setwd(int.wd)
all.data = read_rds(paste0("joinedData_", stage1.date,".Rdata")) %>% 
  mutate(year=year(date), month=month(date))%>% lazy_dt()
prep.all= all.data  %>% 
  group_by(OBJECTID,date, year, month) %>% 
  dplyr::summarise(med_dw_m = median(dom_wv_lake_m),
           med_R_m = median(Red_mean),
           med_B_m = median(Blue_mean),
           med_G_m = median(Green_mean),
           med_Nir_m = median(Nir_mean),
           med_Gb_m = median(Gb_ratio_mean),
           med_Ndssi_m = median(Ndssi_mean),
           med_Nsmi_m = median(Nsmi_mean),
           med_Nsmi_mod_m = median(Nsmi_mod_mean),
           med_Ndti_m = median(Ndti_mean),
           med_NirSwir1_m = median(NirSwir1_mean),
           med_NirGreen_m = median(NirGreen_mean),
           med_BlueNir_m = median(BlueNir_mean),
          med_NirRedBlue_m = median(NirRedBlue_mean))  %>% ungroup() %>%
  dplyr::select(c("OBJECTID","year","month","date" ,all_of(important.cols))) %>% drop_na() %>% 
  as_tibble()


# Apply the classification and convert resulting dataframe in to a spatial object
set.seed(500)
all.classified = cbind(predict(fitted.wf.rf, prep.all), prep.all) %>% as_tibble() %>% 
  mutate(doy=yday(date)) %>% 
  left_join(lakes.sf %>% dplyr::select(-count), by="OBJECTID") %>% arrange(year, month)


# import shoreline and buffer it by 5km, then remove lakes in that buffer, and filter them out of the results
setwd("C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/shapeFiles")
shoreline = st_read("MackenzieShorelineUTM8N.shp")
shoreline.buffer = st_buffer(shoreline, dist = 10000) # distance is in the units of the projection (m)
intersect.list = st_intersects(lakes.sf %>% st_transform("EPSG:32608"), shoreline.buffer)
intersect.lakes =lakes.sf[lengths(intersect.list)>0,]

all.classified.filter = subset(all.classified, !(OBJECTID %in% intersect.lakes$OBJECTID))

setwd(int.wd)
write_rds(all.classified.filter,paste0("final.class_", stage3.date, ".Rdata"))

```



# Plot connectivity results - monthly data need to update to the individual image classification
```{r}
# Plot results
## set color palette
cols= c("2"="#F8766D", "1"="#00BA38" , "0"="#619CFF")
crs.plot = "+proj=tcea +lon_0=-134.3847656 +datum=WGS84 +units=m +no_defs"

setwd(int.wd)
all.classified.filter=readRDS(paste0("final.class_", stage3.date, ".Rdata"))

# import river centerlines 
setwd(shapeFiles.filePath)
## study area and mack.basin are used to select river centerlines in the delta itself
study.area = st_bbox(all.classified.filter %>% st_as_sf(), crs=4326) 
study.area = st_as_sfc(study.area) %>% 
  st_transform(crs = crs.plot)
mack.basin = st_read(import.sword) %>% 
  st_transform(crs = crs.plot) %>% 
  st_intersection(study.area) %>% dplyr::filter(width>90)
## study.area.large and mack.basin.large include centerlines in a slightly larger area that encapsulates the WSC gage Mack River @ Arctic Red River
study.area.large=cbind.data.frame(lon=c(-136.80, -136.80, -133.47, -133.47), 
                 lat=c(67.25, 69.55, 69.55, 67.46)) %>% 
  st_as_sf(coords=c("lon", "lat")) %>% st_set_crs(4326) %>% st_bbox() %>% st_as_sfc() %>% 
  st_transform(crs = crs.plot)
mack.basin.large = st_read(import.sword) %>% 
  st_transform(crs = crs.plot) %>% 
  st_intersection(study.area.large) %>% dplyr::filter(width>90)

# Plot median june and july results

plot.prep = all.classified.filter %>% lazy_dt() %>% 
  mutate(.pred_class=as.numeric(as.character(.pred_class)))  %>% group_by(OBJECTID, month) %>% 
  summarise(mean.score=mean(.pred_class, na.rm=T),
            med.score = median(.pred_class, na.rm=T),
            sd.score = sd(.pred_class),
            count= n() ) %>% dplyr::filter(count>=10) %>% ungroup() %>% 
  as_tibble() %>% 
  left_join(lakes.sf %>% dplyr::select(-count), by="OBJECTID") %>% st_as_sf() %>% 
  st_transform(crs = crs.plot) %>% 
  dplyr::filter(month %in% c(5, 6, 7, 8, 9)) 


plot.prep.gather = plot.prep %>% select(OBJECTID, mean.score, sd.score, geometry, month) %>% gather(Key, Value, -OBJECTID, -month, -geometry) %>% 
  mutate(monthPlot = case_when(
    month==5 ~ factor("May", levels=c("May","June", "July", "August", "September")),
    month==6 ~ factor("June", levels=c("May","June", "July", "August", "September")),
    month==7 ~ factor("July", levels=c("May","June", "July", "August", "September")),
    month==8 ~ factor("August", levels=c("May","June", "July", "August", "September")),
    month==9 ~ factor("September", levels=c("May","June", "July", "August", "September"))
  ) )%>% filter(month!=5)

scale_params <- tibble::tibble(
   monthPlot = factor("June", levels = c("May","June", "July", "August", "September")),
   Key ="mean.score"
  ) 

category.labels = as_labeller(c(mean.score="Mean Connectivity", sd.score="Std. Dev. Connectivity"))

plot.prep.gather %>% filter(Key=="sd.score") %>% 
  ggplot()+ geom_sf(aes(fill=Value), color=NA)+
  facet_wrap(~monthPlot, nrow=2)+theme_bw()+
  annotation_scale(text_cex = 0.9, text_col="black", data=scale_params)+
  scale_fill_viridis_c(option="plasma", limits=c(0,1), oob=squish,
                       labels=c(0, 0.25, 0.5, 0.75,1))+
  geom_sf(data=mack.basin.large, color="grey65", size=0.01)+
  theme(axis.text = element_text(angle=45, hjust=1, size=10),
        strip.text = element_text(size=10, face="bold"),
      #  legend.title = element_text(size=14, face="bold"),
        legend.text=element_text(size=10), 
        panel.background = element_rect(fill="grey95"),
        panel.grid = element_line(color="grey100"),
        plot.background = element_rect(fill="transparent", colour=NA),
        legend.position="bottom",
        legend.key.width = unit(0.60, "in"),
        legend.title = element_text(size=10, face="bold"))+
  labs(fill="Standard Deviation of Connectivity")+
  guides(fill = guide_colourbar(title.position="top", title.hjust = 0.5))

setwd(images.wd)
ggsave("sdLakes_20230309.png",device = "png", width=5.5, height=8, units="in") 

plot.prep.gather %>% filter(Key=="mean.score") %>% 
  ggplot()+ geom_sf(aes(fill=Value), color=NA)+
  facet_wrap(~monthPlot, nrow=2)+theme_bw()+
  annotation_scale(text_cex = 0.9, text_col="black", data=scale_params)+
  scale_fill_viridis_c( limits=c(0,2), oob=squish,
                       labels=c(0, 0.5, 1, 1.5,2))+
  geom_sf(data=mack.basin.large, color="grey65", size=0.01)+
  theme(axis.text = element_text(angle=45, hjust=1, size=10),
        strip.text = element_text(size=10, face="bold"),
      #  legend.title = element_text(size=14, face="bold"),
        legend.text=element_text(size=10), 
        panel.background = element_rect(fill="grey95"),
        panel.grid = element_line(color="grey100"),
        plot.background = element_rect(fill="transparent", colour=NA),
        legend.position="bottom",
        legend.key.width = unit(0.60, "in"),
        legend.title = element_text(size=10, face="bold"))+
  labs(fill="Mean Connectivity Score")+
  guides(fill = guide_colourbar(title.position="top", title.hjust = 0.5))

setwd(images.wd)
ggsave("meanLakes_20230309.png",device = "png", width=5.5, height=8, units="in") 


  
# plot number of observations for each lake in each month
## Plot of number of june observations as a map
plot.uncert.prep = all.classified.filter %>% 
  mutate(.pred_class = as.numeric(as.character(.pred_class))) %>% 
  as_tibble() %>% dplyr::select(-geometry)%>% 
  group_by(OBJECTID, month) %>%count() %>% ungroup() %>% 
  left_join(lakes.sf, by="OBJECTID") %>% st_as_sf() %>% 
  st_transform(crs = crs.plot) %>% 
  mutate(monthWord = case_when(month==5~"May",month==6 ~"June", month==7 ~"July", month==8~"August",month==9~"September"))

plot.uncert.prep$monthWord = factor(plot.uncert.prep$monthWord, levels=c("May","June", "July", "August", "September"))


  scale_params <- tibble::tibble(
    monthWord = factor("May", levels = c("May","June", "July", "August", "September"))
  )




ggplot(data=plot.uncert.prep)+
    geom_sf(aes(fill=n), color=NA)+
    theme_bw()+
    annotation_scale(text_cex = 0.9,
                     data=scale_params)+
    geom_sf(data=mack.basin, color="grey65")+
    scale_fill_viridis_c(begin=0, end=1, direction=1, limits=c(0,100), oob=squish)+
    theme(axis.text.x = element_text(angle=45, hjust=1, size=10),
          #strip.text = element_text(size=14, face="bold"),
          title = element_text(size=14, face="bold"),
          axis.text.y = element_text(size=10),
          legend.title = element_text(size=12, face="bold"),
          legend.text=element_text(size=12),
          legend.position = c(1, 0.05), legend.justification = c(1, 0),
          legend.title.align = 1)+labs(fill="Total Number\nof Observations")+
  guides(fill=guide_colourbar(direction="vertical",  title.position = "top", title.hjust = 0.5))+facet_wrap(~monthWord, nrow=2)

setwd(images.wd)
ggsave("MonthlyUncertMap_20230309.png", height=8, width=8, units="in")


## Plot number of observations in each month as a bar chart
month.labs = c("April", "May","June", "July", "August", "September")
names(month.labs)=c(4,5,6,7,8,9)
all.classified.filter %>% 
  mutate(.pred_class = as.numeric(as.character(.pred_class))) %>% 
  as_tibble() %>% dplyr::select(-geometry)%>% 
  group_by(OBJECTID, month) %>%count() %>% 
  ggplot()+geom_histogram(aes(x=n), binwidth=1, fill="#EFA292", color="grey60")+
  facet_wrap(~month, nrow=5, labeller = labeller(month=month.labs))+
  theme_bw()+
  xlab("number of observations")+ylab("number of lakes")+
  theme(axis.text.x = element_text(size=12),
          #strip.text = element_text(size=14, face="bold"),
          title = element_text(size=14, face="bold"),
          axis.text.y = element_text(size=12),
          legend.title = element_text(size=14, face="bold"),
          legend.text=element_text(size=14),
        strip.text = element_text(size=12, face="bold"))+ylim(0, 3000)


all.classified.filter %>% filter(month == 6) %>% group_by(OBJECTID, month, year) %>% 
  summarise(mean.con = mean(as.numeric(as.character(.pred_class)))) %>% 
  left_join(lakes.sf, by="OBJECTID") %>% st_as_sf() %>% st_transform(crs.plot) %>% 
  ggplot()+geom_sf(aes(fill=mean.con), color=NA)+facet_wrap(~year, nrow=4)+scale_fill_viridis_c()+
  theme(axis.text.x=element_text(angle=45, hjust=1))

```

# functional connectivity vs structural connectivity
```{r}
lesack.marsh.large.ids = c(3243679, 3246340,3244329, 3305102)

setwd(int.wd)
all.classified.filter=read_rds(paste0("final.class_", stage3.date, ".Rdata"))
filt.classified = all.classified.filter %>% 
  filter(OBJECTID %in% lesack.marsh.large.ids) %>% 
  mutate(lm.name = case_when(
    OBJECTID== 3244329 ~ "L80",
    OBJECTID== 3305102 ~ "L129",
    OBJECTID==3243679 ~ "L527a",
    OBJECTID == 3246340 ~ "L278"
  )) %>% filter(month==6)
filt.count = filt.classified %>% group_by(lm.name) %>% count() %>% rename(total.obs = n)

filt.final = filt.classified %>% group_by(lm.name, .pred_class) %>% count() %>% 
  left_join(filt.count, by="lm.name")%>%
  mutate(pct = n/total.obs) %>% 
  mutate(spring.sill.m = case_when(
    lm.name =="L80" ~ 2.6,
    lm.name =="L129" ~ 2.363,
    lm.name =="L527a" ~5.169,
    lm.name == "L278" ~ 4.007
  )) %>% 
  mutate(summer.sill.m = case_when(
    lm.name =="L80" ~ 1.620,
    lm.name =="L129" ~ 1.272,
    lm.name =="L527a" ~4.920,
    lm.name == "L278" ~ 3.5
  )) %>% 
  mutate(mean.conn.time.dyr = case_when(
    lm.name =="L80" ~ 121.4,
    lm.name =="L129" ~ 159.4,
    lm.name =="L527a" ~4.5,
    lm.name == "L278" ~ 17.0
  )) %>% 
  mutate(mean.r.water.content = case_when(    
    lm.name =="L80" ~ 0.928,
    lm.name =="L129" ~ 0.947,
    lm.name =="L527a" ~0.396,
    lm.name == "L278" ~ 0.792))


filt.final


  ggplot(data=filt.final)+
    geom_bar(aes(x=reorder(lm.name, -mean.conn.time.dyr), y=pct, fill=.pred_class), 
                    stat="identity", position="dodge")+
  ylab("percent of observations")+
  geom_text(aes(x=lm.name, y=0.75, 
                label=paste0("n=",total.obs)))+theme_bw()+
    geom_text(aes(x=lm.name, y=0.68, label = 
                    paste0("spring sill\nelevation: ", 
                           spring.sill.m, "m")))+
    geom_text(aes(x=lm.name, y=0.58, label = 
                    paste0("summer sill\nelevation: ", 
                           summer.sill.m, "m")))+
    geom_text(aes(x=lm.name, y=0.46, label = 
                    paste0("mean connection\ntime (d/yr):\n", 
                           mean.conn.time.dyr)))+
    geom_text(aes(x=lm.name, y=0.34, label = 
                    paste0("river water content:\n", 
                           mean.r.water.content)))+xlab("lake names from Lesack & Marsh, 2010")+
    ggtitle("Functional connectivity classification 2000-2022\ncompared to structural connectivity from Lesack & Marsh (2010)\n1964|1972-2005 in the month of June")
setwd(images.wd)
ggsave("largeLakesLesackMarsh2010.png", width =8, height = 6, units = "in" )

# Are there times when we have lakes in a given year go from low to high connectivity? If so, what is the water level at the nearest gage station?

  
```


# Study Area Figure
```{r}
# Prep for the large map
n.am <- sfheaders::sf_multipolygon(obj = map_data("world"),
                             multipolygon_id ="group",
                             x="long", y="lat",
                             keep = T) %>% 
  st_set_crs(4326) %>% filter(region=="Canada" | region=="USA") %>% 
  st_transform("ESRI:102008") 

coord.prep = st_coordinates(n.am) %>% as_tibble()

min.x = -4593551
max.x = max(coord.prep$X)
min.y = 0
max.y = 4017149

point = st_centroid(study.area.large) %>% st_transform("ESRI:102008")

largescale.map = ggplot()+
  geom_sf(data=n.am %>% 
            st_crop(xmin = min.x, xmax =max.x , ymin = min.y, ymax = max.y), 
          fill="grey20", color="grey50", size=0.1)+theme_bw()+
  geom_sf(data=point, color="#0096FF", size=3)+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.background = element_rect(fill="grey90"),
        plot.background = element_rect(fill="transparent", color=NA))+
  coord_sf(expand = FALSE)



# plot the zoom in map
setwd(int.wd)
lakes=read_rds(paste0("final.class_", stage3.date, ".Rdata")) %>% group_by(OBJECTID) %>% 
  select(OBJECTID, geometry) %>% distinct() %>% st_as_sf()

mack.delta <- sfheaders::sf_multipolygon(obj = map_data("world"),
                             multipolygon_id ="group",
                             x="long", y="lat",
                             keep = T) %>% 
  st_set_crs(4326) %>% filter(region=="Canada" | region=="USA") %>% 
  st_transform(crs.plot) %>% st_crop(xmin = -150941.35, ymin = 7486525.66,
                                     xmax = 39377.58, ymax = 7744125.65 )

zoom.map = ggplot()+
 # coord_sf(crs = st_crs())  # force the ggplot2 map to be in 3857
  geom_sf(data=mack.delta %>% st_transform(crs.plot), fill="darkseagreen", color="darkseagreen4")+
  geom_sf(data=lakes %>% st_transform(crs.plot), fill="grey99", color=NA, inherit.aes = F)+
  geom_sf(data=mack.basin.large%>% st_transform(crs.plot) %>% 
            st_intersection(mack.delta %>% st_transform(crs.plot)),
          color="cadetblue")+theme_bw()+
  theme(axis.text.x=element_text(angle=45, hjust = 1, size=12),
        axis.text.y=element_text(size=12),
        panel.background = element_rect(fill="lightblue"),
        panel.grid = element_line(colour="#7393B3"),
        legend.position="none")+
  annotation_scale(text_col="white")+
  coord_sf(expand = FALSE)



ggdraw() +
  draw_plot(zoom.map) +
  draw_plot(largescale.map,
    height = 0.2,
    x = -0.11,
    y = 0.18
  )
setwd(images.wd)
ggsave("studyArea.pdf", device="pdf", width=4, height=5, units="in")
```


# Analyze discharge and connectivity 
```{r}
## Import classification results 
setwd(int.wd)
import.data =  readRDS(paste0("final.class_", stage3.date, ".Rdata")) %>% 
  dplyr::select(.pred_class, OBJECTID, year, month, date, doy)


# import rivers for plotting purposes
crs.plot="+proj=tcea +lon_0=-134.3847656 +datum=WGS84 +units=m +no_defs"
crs.analyze = "EPSG:32608"
setwd(shapeFiles.filePath)
study.area.large=cbind.data.frame(lon=c(-136.80, -136.80, -133.47, -133.47), 
                 lat=c(67.25, 69.55, 69.55, 67.46)) %>% 
  st_as_sf(coords=c("lon", "lat")) %>% st_set_crs(4326) %>% st_bbox() %>% st_as_sfc() %>% 
  st_transform(crs = crs.plot)
mack.basin.large = st_read(import.sword) %>% 
  st_transform(crs = crs.plot) %>% 
  st_intersection(study.area.large) %>% dplyr::filter(width>90)


# Import the Mackenzie discharge at arctic red river and calculate maximum an mean flow in each month

rr.level = hy_daily_levels(station_number = "10LC014") %>% 
  mutate(year=year(Date), month=month(Date),doy = yday(Date)) %>% 
   filter(month>=5 & month<=9) 

rr.location = hy_stations(station="10LC014") %>% 
  st_as_sf(coords=c("LONGITUDE","LATITUDE")) %>% st_set_crs(4326) %>% 
  mutate(rivr = "Mackenzie River at Arctic Red River") %>% 
  st_transform(crs = crs.analyze)

# Import the Peel River Discharge and calculate maximum & mean flow in each month
pr.level = hy_daily_levels(station_number = "10MC002") %>% mutate(year=year(Date), month=month(Date),
                                                       doy = yday(Date)) %>% 
  filter(month>=5 & month<=9) 

pr.location = hy_stations(station="10MC002") %>% 
  st_as_sf(coords=c("LONGITUDE","LATITUDE")) %>% st_set_crs(4326) %>% 
  mutate(rivr = "Peel River above Fort McPhearson") %>% 
  st_transform(crs = crs.analyze)

# # Import the Peel Channel Discharge above Aklavik and calculate maximum & mean flow in each month
pc.level = hy_daily_levels(station_number = "10MC003") %>% mutate(year=year(Date), month=month(Date),
                                                       doy = yday(Date)) %>% 
  filter(year>=2009 & year !=2016) %>% filter(month>=5 & month<=9)

pc.location = hy_stations(station="10MC003") %>% 
  st_as_sf(coords=c("LONGITUDE","LATITUDE")) %>% st_set_crs(4326) %>% 
  mutate(rivr = "Peel Channel Above Aklavik") %>% 
  st_transform(crs = crs.analyze)

# # Import the East Channel at Inuvik and calculate maximum & mean flow in each month
eci.level = hy_daily_levels(station_number = "10LC002") %>% mutate(year=year(Date), month=month(Date),
                                                       doy = yday(Date)) %>% 
  filter(year <=2017) %>% filter(month>=5 & month<=9) %>% group_by(month, year, STATION_NUMBER) 

eci.location = hy_stations(station="10LC002") %>% 
  st_as_sf(coords=c("LONGITUDE","LATITUDE")) %>% st_set_crs(4326) %>% 
  mutate(rivr = "East Channel at Inuvik") %>% 
  st_transform(crs = crs.analyze)

# Import Middle Channel below Raymond Channel and calculate maximum & mean flow in each month
mcr.level = hy_daily_levels(station_number = "10MC008") %>% mutate(year=year(Date), month=month(Date),
                                                       doy = yday(Date)) %>% 
  filter(year>=2009 & year <=2016) %>% filter(month>=5 & month<=9) %>% group_by(month, year,STATION_NUMBER )

mcr.location = hy_stations(station="10MC008") %>% 
  st_as_sf(coords=c("LONGITUDE","LATITUDE")) %>% st_set_crs(4326) %>% 
  mutate(rivr = "Middle Channel below Raymond Channel") %>% 
  st_transform(crs = crs.analyze)


# Import Napoiak Channel above Shallow Bay and calculate maximum & mean level in each month
nc.level = hy_daily_levels(station_number = "10MC023") %>% mutate(year=year(Date), month=month(Date),
                                                       doy = yday(Date)) %>% 
  filter(year %in% c(2001, 2005, 2006, 2009, 2010, 2011, 2012, 2014, 2015, 2016)) %>% 
  filter(month>=5 & month<=9) 

nc.location = hy_stations(station="10MC023") %>% 
  st_as_sf(coords=c("LONGITUDE","LATITUDE")) %>% st_set_crs(4326) %>% 
  mutate(rivr = "Napoiak Channel above Shallow Bay") %>% 
  st_transform(crs = crs.analyze)

nc.level %>% 
  ggplot()+geom_line(aes(x=doy,y=Value, group=year, color=as.factor(year)))

# East channel at the coast
ecc.level = hy_daily_levels(station_number = "10LC013") %>% 
  mutate(year=year(Date), month=month(Date),doy = yday(Date)) %>% 
  filter() %>% 
  filter(month>=5 & month<=9) 
ecc.level %>% ggplot()+geom_line(aes(x=doy, y=Value))+facet_wrap(~year)

ecc.location = hy_stations(station="10LC013") %>% 
  st_as_sf(coords=c("LONGITUDE","LATITUDE")) %>% st_set_crs(4326) %>% 
  mutate(rivr = "MACKENZIE RIVER (EAST CHANNEL) ABOVE KITTIGAZUIT BAY") %>% 
  st_transform(crs = crs.analyze)


# KULUARPAK CHANNEL at the coast
kc.level = hy_daily_levels(station_number = "10LC021") %>% 
  mutate(year=year(Date), month=month(Date),doy = yday(Date)) %>% 
  filter() %>% 
  filter(month>=5 & month<=9) 
kc.level %>% ggplot()+geom_line(aes(x=doy, y=Value))+facet_wrap(~year)

kc.location = hy_stations(station="10LC021") %>% 
  st_as_sf(coords=c("LONGITUDE","LATITUDE")) %>% st_set_crs(4326) %>% 
  mutate(rivr = "MACKENZIE RIVER AT KULUARPAK CHANNEL") %>% 
  st_transform(crs = crs.analyze)


#Reindeer channel
rc.level = hy_daily_levels(station_number = "10MC011") %>% 
  mutate(year=year(Date), month=month(Date),doy = yday(Date)) %>% 
  filter() %>% 
  filter(month>=5 & month<=9) 
rc.level %>% ggplot()+geom_line(aes(x=doy, y=Value))+facet_wrap(~year)

rc.location = hy_stations(station="10MC011") %>% 
  st_as_sf(coords=c("LONGITUDE","LATITUDE")) %>% st_set_crs(4326) %>% 
  mutate(rivr = "MACKENZIE RIVER (REINDEER CHANNEL) AT ELLICE ISLAND") %>% 
  st_transform(crs = crs.analyze)

#MACKENZIE RIVER (MIDDLE CHANNEL) AT TUNUNUK POINT
tp.level = hy_daily_levels(station_number = "10LC012") %>% 
  mutate(year=year(Date), month=month(Date),doy = yday(Date)) %>% 
  filter(year !=2013 & year!=2012) %>% 
  filter(month>=5 & month<=9) 
tp.level %>% ggplot()+geom_line(aes(x=doy, y=Value))+facet_wrap(~year)

tp.location = hy_stations(station="10LC012") %>% 
  st_as_sf(coords=c("LONGITUDE","LATITUDE")) %>% st_set_crs(4326) %>% 
  mutate(rivr = "MACKENZIE RIVER (MIDDLE CHANNEL) AT TUNUNUK POINT") %>% 
  st_transform(crs = crs.analyze)



# combine data together
## Summary Data
flow.df = rbind.data.frame(rr.level, pr.level, pc.level,eci.level, mcr.level, nc.level, ecc.level, kc.level, rc.level, tp.level) %>% filter(year>=2000)
flow.df.all = rbind.data.frame(rr.level, pr.level, pc.level,eci.level, mcr.level, nc.level, ecc.level, kc.level, rc.level, tp.level)
## location data
flow.location.df = rbind.data.frame(rr.location, pr.location, pc.location, eci.location, mcr.location, nc.location, ecc.location, kc.location,rc.location, tp.location)



#Import and join island and lake geometries
## 1. join lakes to island
# setwd(pil.wd)
# mack.islands = st_read("MackenzieDeltaIslands.shp") %>% 
#   dplyr::select(fid, geometry) %>% 
#   st_transform(crs.analyze)
# mack.islands$area = units::set_units(st_area(mack.islands), km^2)
# 
# islands.join = lakes.sf %>% st_transform(crs.analyze) %>% st_join(mack.islands) %>% filter(!is.na(fid)) 
# 
# install.packages("PerformanceAnalytics")
 library(PerformanceAnalytics)




#good.stations = c("10LC002", '10LC014',"10MC002")

# ### regional correlation analysis based on islands
# good.fids = import.data %>% 
#   select(.pred_class, OBJECTID, date) %>% 
#   mutate(.pred_class = as.numeric(as.character(.pred_class))) %>% 
#   left_join(islands.join %>% select(fid, OBJECTID, geometry), by="OBJECTID") %>% 
#   group_by(fid,date) %>% 
#   count() %>% ungroup() %>% 
#   group_by(fid) %>% 
#   summarise(max.count = max(n))
# 
# 
# correl.prep.all = import.data %>% 
#   select(.pred_class, OBJECTID, date) %>% 
#   mutate(.pred_class = as.numeric(as.character(.pred_class))) %>% 
#   left_join(islands.join %>% select(fid, OBJECTID, geometry), by="OBJECTID") %>% 
#   group_by(fid,date) %>% 
#   summarise(mean.val = mean(.pred_class),
#             med.val = median(.pred_class), 
#             count=n()) %>% 
#   left_join(good.fids, by="fid") %>%
#   mutate(pct.visible = count/max.count) %>% 
#   filter(pct.visible>0.5 & max.count>10) %>% 
#   mutate(date.7 = date-days(7),
#          date.14 = date-days(14)) %>% 
#   left_join(flow.df.all %>% rename(date=Date, Value.0 = Value) %>% 
#               select(STATION_NUMBER, Value.0, date), by=c("date")) %>% 
#   left_join(flow.df.all %>% rename(date.7=Date, Value.7 = Value) %>% 
#               select(STATION_NUMBER, Value.7, date.7), by=c("date.7", "STATION_NUMBER")) %>% 
#   left_join(flow.df.all %>% rename(date.14=Date, Value.14 = Value) %>% 
#               select(STATION_NUMBER, Value.14, date.14), by=c("date.14", "STATION_NUMBER")) %>% 
#   select(-date.7, -date.14) %>% 
#   group_by(fid, STATION_NUMBER) %>% nest() %>% ungroup() %>% filter(!is.na(STATION_NUMBER))
# 
# 
# row.combo.all=NULL
# for (i in 1:length(correl.prep.all$fid)){
#   dat=correl.prep.all$data[[i]] %>% na.omit()
#   fid = correl.prep.all$fid[[i]]
#   stat = correl.prep.all$STATION_NUMBER[[i]]
#   len = length(dat$date)
#   #if(length(dat$date)<40){next}
#   cor.0 = cor.test(dat$Value.0, dat$mean.val, method = "kendall", alternative="greater")
#   coeff.0 = cor.0$estimate[[1]]
#   pval.0 = cor.0$p.value
#   cor.7 = cor.test(dat$Value.7, dat$mean.val, method = "kendall",alternative="greater")
#   coeff.7 = cor.7$estimate[[1]]
#   pval.7 = cor.7$p.value
#   cor.14 = cor.test(dat$Value.14, dat$mean.val, method = "kendall", alternative="greater")
#   coeff.14 = cor.14$estimate[[1]]
#   pval.14 = cor.14$p.value
#   col.combo = cbind.data.frame(fid, stat, len, coeff.0, pval.0, coeff.7, pval.7, coeff.14, pval.14)
#   row.combo.all = rbind.data.frame(row.combo.all, col.combo)
# }
# 
# prep.all = row.combo.all %>% as_tibble() %>% select(-len) %>% 
#   rename(STATION_NUMBER=stat) %>% 
#   rownames_to_column() %>% 
#   gather(column, value, -rowname, -fid, -STATION_NUMBER, -pval.0, -pval.14, -pval.7) %>% 
#   mutate(pval = case_when(
#     column=="coeff.0"~ pval.0,
#     column=="coeff.7"~ pval.7,
#     column=="coeff.14"~ pval.14
#   )) %>% select(-pval.0, -pval.14,-pval.7, -rowname) %>% 
#   mutate(lag.days = as.numeric(sub(".*\\.","", column))) %>% 
#   left_join(mack.islands %>% select(fid, geometry), by="fid") %>% st_as_sf() %>% 
#   mutate(lag.text = case_when(
#   lag.days == 0 ~ "0 days",
#   lag.days == 7 ~ "7 days",
#   lag.days == 14 ~ "14 days")) %>% 
#   left_join(flow.location.df %>% as_tibble() %>% 
#               select(STATION_NUMBER, STATION_NAME), by="STATION_NUMBER") 
# 
# 
# prep.all.sum = prep.all %>% filter(pval<0.05 & value>=0)%>% group_by(fid, STATION_NAME, STATION_NUMBER) %>% 
#   summarise(max.value = max(value),
#             pval = pval[which.max(value)]) %>%
#   select(fid, STATION_NAME, 
#          STATION_NUMBER, max.value, geometry, pval) %>% ungroup() %>% 
#   mutate(lag.text="maximum positive\ncorrelation") %>% 
#   rename(value=max.value)
# 
# prep.all.sum.all = prep.all %>% filter(pval<0.05)%>% group_by(fid, STATION_NAME, STATION_NUMBER) %>% 
#   summarise(max.value = max(value),
#             pval = pval[which.max(value)]) %>%
#   select(fid, STATION_NAME, STATION_NUMBER, max.value, geometry, pval) %>% ungroup() %>% 
#   mutate(lag.text="maximum positive\ncorrelation") %>% rename(value=max.value)
# 
# prep.all.combo = rbind.data.frame(prep.all %>% select(-column, -lag.days), prep.all.sum, prep.all.sum.all)
# 
# prep.all.combo$lag.text = factor(prep.all.combo$lag.text, 
#                            levels=c("0 days", "7 days", "14 days", "maximum positive\ncorrelation"))
# 
# 
# library(colorspace)
# ggplot()+
#   # geom_sf(data = prep.all.combo, # %>% filter(STATION_NUMBER %in% good.stations), 
#   #         fill="grey50", color=NA)+
#   geom_sf(data=prep.all.combo %>% 
#             #filter(STATION_NUMBER %in% good.stations) %>% 
#             filter(pval<=0.05 & value>=0), 
#           aes(fill=value), color=NA)+
#   geom_sf(data=mack.basin.large, color="grey30", size=0.2 )+theme_bw()+
#   geom_sf(data=flow.location.df,# %>% 
#           #  filter(STATION_NUMBER %in% good.stations), 
#           color="black", size=5)+
#     facet_grid(vars(lag.text), vars(STATION_NAME),
#                labeller = label_wrap_gen(width = 15, multi_line = TRUE))+
#   scale_fill_continuous_sequential(
#                        guide=guide_colorbar(barheight = 20),
#                        palette="YlGn", limits=c(0,0.65))+
#   theme(axis.text.x = element_text(angle=45, hjust=1, size=12))+
#    theme(panel.background = element_rect(fill="grey95"),
#         panel.grid = element_line(color="grey100"),
#           title = element_text(size=12, face="bold"),
#           axis.text.y = element_text(size=12),
#           legend.title = element_text(size=14, face="bold"),
#           legend.text=element_text(size=14),
#         strip.text = element_text(size=12, face="bold")
#     )+labs(fill="Kendall's\nTau")
# setwd(images.wd)
# ggsave("dischargeIslandCorr.png", width=18, height=18, units="in")
# 
# #plot max for all
# prep.all.sum2 = prep.all %>% filter(pval<0.05 & value>=0)%>% group_by(fid) %>% 
#   summarise(max.value = max(value),
#             pval = pval[which.max(value)]) %>%
#   select(fid,max.value, geometry, pval) %>% ungroup() %>% 
#   mutate(lag.text="maximum positive\ncorrelation") %>% 
#   rename(value=max.value)
# ggplot()+
#   geom_sf(data=prep.all.sum2 , 
#           aes(fill=value), color=NA)+
#   geom_sf(data=mack.basin.large, color="grey30", size=0.2 )+theme_bw()+
#   scale_fill_continuous_sequential(
#                        guide=guide_colorbar(barheight = 20),
#                        palette="YlGn", limits=c(0,0.65))+
#   theme(axis.text.x = element_text(angle=45, hjust=1, size=12))+
#    theme(panel.background = element_rect(fill="grey95"),
#         panel.grid = element_line(color="grey100"),
#           title = element_text(size=12, face="bold"),
#           axis.text.y = element_text(size=12),
#           legend.title = element_text(size=14, face="bold"),
#           legend.text=element_text(size=14),
#         strip.text = element_text(size=12, face="bold")
#     )+labs(fill="Kendall's\nTau")
# setwd(images.wd)
# ggsave("dischargeIslandMaxCorr.png", width=8, height=9, units="in")


# OK, lets just look at september in terms of storm surge in 2015 227 is aug 15th, 273 is end of september
flow.df %>% 
  filter(doy>227&doy<273 & Parameter=="Level" &year==2015 & 
           STATION_NUMBER != "10MC011" &
           STATION_NUMBER != "10LC014" &
           STATION_NUMBER != "10MC002") %>% 
  left_join(flow.location.df, by="STATION_NUMBER") %>% 
  ggplot()+geom_line(aes(x=doy, y=Value, group=year))+
  facet_wrap(~str_wrap(STATION_NUMBER, 20),  scales="free_y")

#manually pick beginning and end of event
flow.df %>% 
  filter(doy>227&doy<260 & Parameter=="Level" &
           STATION_NUMBER=="10LC021") %>% 
  ggplot()+
    geom_line(aes(x=doy, y=Value, group=year, color=as.factor(year)))+
  geom_point(aes(x=doy, y=Value, color=as.factor(year)))

# 10LC002 first valley 238, first peak 240, second valley  242, peak 244
wsc.10LC002= data.frame(
  first.valley.doy = 238,
  first.peak.doy = 240,
  second.valley.doy = 242,
  main.peak = 244,
  approx.end.doy = 250,
  STATION_NUMBER = "10LC002"
)
wsc.10LC012= data.frame(
  first.valley.doy = 238,
  first.peak.doy = 240,
  second.valley.doy = 242,
  main.peak = 244,
  approx.end.doy = 250,
  STATION_NUMBER="10LC012"
)
wsc.10LC013= data.frame(
  first.valley.doy = 237,
  first.peak.doy = 239,
  second.valley.doy = 241,
  main.peak = 244,
  approx.end.doy = 247,
  STATION_NUMBER="10LC013"
)
wsc.10LC021= data.frame(
  first.valley.doy = 237,
  first.peak.doy = 239,
  second.valley.doy = 241,
  main.peak = 244,
  approx.end.doy = 247,
  STATION_NUMBER="10LC021"
)
wsc.10MC003= data.frame(
  first.valley.doy = 238,
  first.peak.doy = 240,
  second.valley.doy = 242,
  main.peak = 244,
  approx.end.doy = NA,
  STATION_NUMBER="10MC003"
)

wsc.10MC008= data.frame(
  first.valley.doy = 238,
  first.peak.doy = 240,
  second.valley.doy = 242,
  main.peak = 244,
  approx.end.doy = 250,
  STATION_NUMBER="10MC008"
)

wsc.10MC023= data.frame(
  first.valley.doy = 238,
  first.peak.doy = 240,
  second.valley.doy = 242,
  main.peak = 244,
  approx.end.doy = 250,
  STATION_NUMBER="10MC023"
)

event.2015 = rbind.data.frame(wsc.10LC002, wsc.10LC012, wsc.10LC013, wsc.10LC021,
                              wsc.10MC003, wsc.10MC008, wsc.10MC023) %>% 
  mutate(year=2015) %>% 
  left_join(flow.df %>% select(doy, Value, STATION_NUMBER, year) %>% rename(first.valley.doy=doy), 
            by=c("STATION_NUMBER", "first.valley.doy", "year")) %>% 
  rename(min.value = Value)%>% 
  left_join(flow.df %>% select(doy, Value, STATION_NUMBER, year) %>% rename(main.peak=doy), 
            by=c("STATION_NUMBER", "main.peak", "year")) %>% 
  rename(peak.value = Value) %>% 
  mutate(dif.level = peak.value-min.value,
         dif.doy = approx.end.doy-first.valley.doy) %>% 
  as_tibble()


p1 = ggplot()+
  geom_sf(data=mack.basin.large, color="grey40")+
  geom_sf(data=event.2015 %>% 
            left_join(flow.location.df, by="STATION_NUMBER") %>% 
            st_as_sf(), aes(color=dif.level), size=3)+
  scale_color_viridis_c()+theme_void()+
  labs(color="difference\nwater level\n(m)")

p2= ggplot()+
  geom_sf(data=mack.basin.large, color="grey40")+
  geom_sf(data=event.2015 %>% 
            left_join(flow.location.df, by="STATION_NUMBER") %>% 
            st_as_sf(), aes(color=first.valley.doy), size=3)+
  scale_color_viridis_c(limits=c(235,255),option="plasma" )+theme_void()+
  labs(color="start doy")

p3= ggplot()+
  geom_sf(data=mack.basin.large, color="grey40")+
  geom_sf(data=event.2015 %>% 
            left_join(flow.location.df, by="STATION_NUMBER") %>% 
            st_as_sf(), aes(color=main.peak), size=3)+
  scale_color_viridis_c(limits=c(235,255),option="plasma")+theme_void()+
  labs(color="peak doy")

p4= ggplot()+
  geom_sf(data=mack.basin.large, color="grey40")+
  geom_sf(data=event.2015 %>% 
            left_join(flow.location.df, by="STATION_NUMBER") %>% 
            st_as_sf(), aes(color=approx.end.doy), size=3)+
  scale_color_viridis_c(limits=c(235, 255),option="plasma")+theme_void()+
  labs(color="approximate\nevent end\ndoy")


p1+p2+p3+p4


flow.df %>% 
  filter(doy>227&doy<273 & Parameter=="Level" &year==2015 & 
           STATION_NUMBER != "10MC011" &
           STATION_NUMBER != "10LC014" &
           STATION_NUMBER != "10MC002") %>% 
  left_join(flow.location.df, by="STATION_NUMBER") %>% 
  ggplot()+geom_line(aes(x=doy, y=Value, group=year))+
    geom_point(data=event.2015,aes(x=first.valley.doy, y=min.value, color="min valley"))+
  geom_point(data=event.2015,aes(x=main.peak, y=peak.value, color="max.peak"))+
    facet_wrap(~str_wrap(STATION_NUMBER, 20),  scales="free_y")

flow.df.all%>% 
  filter((month==9 |month==8)) %>% 
  ggplot()+geom_line(aes(x= doy, y=Value, color=factor(year), group=year))+
  facet_grid(vars(STATION_NUMBER), vars(factor(year)), scales="free_y")
  


prep.test =all.classified.filter %>% filter((month==9)) %>% 
  group_by(OBJECTID, month, year) %>% 
  summarise(mean.con=mean(as.numeric(as.character(.pred_class))),
            count=n()) %>%  
  ungroup() %>% left_join(lakes.sf, by="OBJECTID") %>% st_as_sf() %>% 
  st_transform(crs.plot)
 
ggplot(data=prep.test)+
  geom_sf(aes(fill=mean.con), color=NA)+
  scale_fill_viridis_c()+
  facet_wrap(~year)+theme_bw()+theme(
    panel.background = element_rect(fill="grey40"),
    panel.grid = element_line(color="grey42"),
    axis.text.x = element_text(angle=45, hjust=1)
  )




# # look at correlation between connectivity and lake level 
# prep.all.sum2 = prep.all %>% 
#   filter(pval<0.05 & value>=0)%>%
#   group_by(fid) %>% 
#   summarise(max.value = max(value),
#             pval = pval[which.max(value)],
#             lag = lag.days[which.max(value)],
#             STATION_NUMBER = STATION_NUMBER[which.max(value)]) %>%
#   select(fid,max.value, geometry, pval, STATION_NUMBER, lag) %>% ungroup() %>% 
#  
#   rename(value=max.value)
# 
# dis.join = islands.join %>% as_tibble() %>% 
#   left_join(prep.all.sum2 %>% as_tibble() %>% select(-geometry), by="fid")
# 
# dis.cor.prep =  all.classified.filter %>% 
#   select(.pred_class, OBJECTID, date, year, month, doy) %>% 
#   mutate(.pred_class = as.numeric(as.character(.pred_class))) %>% 
#   left_join(dis.join, by="OBJECTID") %>% filter(!is.na(STATION_NUMBER))
# dis.cor.prep.update = dis.cor.prep %>% 
#   select(.pred_class, OBJECTID, date, year, month, fid, value, pval,
#          lag, STATION_NUMBER) %>% 
#   mutate(date.7 = date-days(7),
#          date.14 = date-days(14)) %>% 
#   left_join(flow.df %>% rename(date=Date, Value.0 = Value) %>% 
#               select(STATION_NUMBER, Value.0, date), 
#             by=c("date", "STATION_NUMBER")) %>% 
#   left_join(flow.df %>% rename(date.7=Date, Value.7 = Value) %>% 
#               select(STATION_NUMBER, Value.7, date.7),
#             by=c("date.7", "STATION_NUMBER")) %>% 
#   left_join(flow.df %>% rename(date.14=Date, Value.14 = Value) %>% 
#               select(STATION_NUMBER, Value.14, date.14), 
#             by=c("date.14", "STATION_NUMBER")) %>% 
#   mutate(final.value = case_when(
#     lag == 0 ~ Value.0,
#     lag==7 ~Value.7,
#     lag==14~Value.14
#   )) %>% select(OBJECTID, .pred_class, 
#                 final.value, date, value, pval) %>% 
#   filter(!is.na(final.value))
# 
# 
# dis.cor.prep.update %>% group_by(OBJECTID, value) %>% 
#   nest() %>%ungroup() %>%  filter(value>0.75) %>% 
#   unnest(data) %>% 
#   ggplot()+geom_point(aes(x=final.value, y=.pred_class))+
#   facet_wrap(~OBJECTID)
# #### maybe the trick is to, for each lake, calculate if each of the three levels of connectivity have statistically significant discharge???  
# dis.cor.nest = dis.cor.prep.update %>% group_by(OBJECTID) %>% nest() %>% ungroup()
# tuk.combo = NULL
# for(g in 1:length(dis.cor.nest$OBJECTID)){
#   test = dis.cor.nest$data[[g]] %>% mutate(.pred_class=as.character(.pred_class))
#   id = dis.cor.nest$OBJECTID[[g]]
#   if(test %>% group_by(.pred_class) %>% count()%>% nrow()==1){
#     tuk.table = data.frame(
#       tuk.rows = c("1-0", "2-0", "2-1"),
#       diff = c(NA, NA, NA), lwr = c(NA, NA, NA), upr = c(NA, NA, NA), 
#       p.adj = c(NA, NA, NA), 
#       OBJECTID = rep(id, 3),
#       class = c("all same class", "all same class", "all same class")
#       
#     )
#   } else{
#     res.aov=aov(final.value~.pred_class, data = test)
#     sum.aov = summary(res.aov)
#     tuk.aov = TukeyHSD(res.aov)
#     tuk.rows = tuk.aov$.pred_class %>% rownames()
#     tuk.table = cbind.data.frame(tuk.rows, tuk.aov$.pred_class %>% as_tibble()) %>% 
#       mutate(OBJECTID=id) %>% mutate(class = ifelse(`p adj` <=0.05, "signif", "not signif")) %>% 
#       as_tibble() %>% rename(p.adj=`p adj`)
#   }
#   
#   tuk.combo = rbind.data.frame(tuk.combo, tuk.table)
# }
# tuk.combo %>% as_tibble() %>% left_join(lakes.sf, by="OBJECTID") %>% 
#   mutate(tuk.rows = case_when(
#     tuk.rows =="1-0" ~ factor("0-1", levels = c("0-1", "1-2","0-2")),
#     tuk.rows =="2-1" ~ factor("1-2", levels = c("0-1", "1-2","0-2")),
#     tuk.rows =="2-0" ~ factor("0-2", levels = c("0-1", "1-2","0-2")),
#   )) %>% 
#   st_as_sf() %>% st_transform(crs.plot) %>% ggplot()+
#   geom_sf(aes(fill=class), color=NA)+facet_wrap(~tuk.rows)+theme_void()+
#   theme(strip.text = element_text(face="bold", size=16),
#         panel.background = element_rect(fill="grey20"), 
#         legend.position = "bottom")+
#   labs(fill="")



### sill elevation calculation
# Calculate added error per meter away from each gage station
flow.prep = flow.df.all %>% 
   select(STATION_NUMBER, Date, Value) %>% 
   spread(STATION_NUMBER, Value) %>% 
   na.omit() %>% select(-Date) %>% colMeans() 
col.names = flow.df.all %>% 
   select(STATION_NUMBER, Date, Value) %>% 
   spread(STATION_NUMBER, Value) %>% 
   na.omit() %>% select(-Date) %>% colnames()
flow.prep2 = cbind.data.frame(flow.prep %>% as_tibble(), col.names) %>% 
  rename(STATION_NUMBER = col.names) %>% 
  left_join(flow.location.df %>% 
              select(STATION_NAME, STATION_NUMBER, geometry), by="STATION_NUMBER") %>% 
  st_as_sf()
## distance matrix
dist.matrix = st_distance(flow.prep2)
dimnames(dist.matrix) = list(col.names, col.names)
dist.df = t(combn(colnames(dist.matrix), 2))
dist.df = data.frame(dist.df, dist = dist.matrix[dist.df])
## difference matrix between water levels
dif.matrix = dist(flow.prep2$value, diag=T, upper=T) %>% as.matrix()
dimnames(dif.matrix) = list(col.names, col.names)
dif.df = t(combn(colnames(dif.matrix), 2))
dif.df = data.frame(dif.df, dist = dif.matrix[dif.df]) %>% rename(dif = dist)
## combine distance and difference matrices
dist.dif.df = dist.df %>% left_join(dif.df, by=c("X1", "X2")) %>% as_tibble()
dist.dif.df %>% 
  filter(X1 != "10MC002" &  X1 != "10LC014") %>% 
  filter(X2 != "10MC002" &  X2 != "10LC014") %>% 
  ggplot()+geom_point(aes(x=dist, y=dif, color=X1))+theme_classic()+
  geom_smooth(aes(x=dist, y= dif), method="lm")+
  xlab("distance (m) between station pairs")+ylab("difference in average\nwater level (m) between station pairs")+labs(color="WSC Station\nNumber")
setwd(images.wd)
ggsave("levelDiff.png", width = 6, height = 5, units = "in")
# filter out the stations that are upstream of the delta, since they behave differently
dist.dif.df.filt = dist.dif.df %>% 
  filter(X1 != "10MC002" &  X1 != "10LC014") %>% 
  filter(X2 != "10MC002" &  X2 != "10LC014")
# Calculate the relationship between distance and difference in water level
dist.dif.mod = lm(dif ~ as.numeric(dist), data =dist.dif.df.filt )
mod.int = dist.dif.mod$coefficients[[1]]
mod.slope = dist.dif.mod$coefficients[[2]]


## For each station select lakes that are within 50km


station.buffer = flow.location.df %>% st_buffer(50000) %>% # buffer by 50km
  select(STATION_NUMBER, STATION_NAME, geometry) %>% 
  filter(STATION_NUMBER != "10MC002" &  STATION_NUMBER != "10LC014") 

lakes.proj = lakes.sf %>% st_transform(crs.analyze) %>% 
  select(OBJECTID, geometry)
lake.list = st_intersects(lakes.proj, station.buffer)
lakes.combo = lakes.proj[lengths(lake.list)>0,] %>% st_join(station.buffer) 
# get distances between each lake and the relevant station
lakes.point = lakes.combo %>% st_centroid()
station.point = lakes.combo %>% as_tibble() %>% select(STATION_NUMBER) %>% 
  left_join(flow.location.df, by="STATION_NUMBER") %>% 
  st_as_sf() %>% select(STATION_NUMBER)

lake.station.dist = st_distance(lakes.point, station.point, by_element=T) %>% as_tibble()

lakes.station.dist = cbind.data.frame(lakes.combo, lake.station.dist) %>% as_tibble() %>% rename(dist.m = value) %>% 
  mutate(dist_error = dist.m * mod.slope)



nest.sill = import.data %>% 
  left_join(lakes.station.dist %>% as_tibble() %>% select(-geometry), by="OBJECTID") %>% 
  filter(!is.na(STATION_NUMBER)) %>% 
  mutate(.pred_class = as.numeric(as.character(.pred_class))) %>% 
  filter(.pred_class !=1) %>%
  left_join(flow.df.all %>% rename(date=Date, Value.0 = Value) %>% 
              select(STATION_NUMBER, Value.0, date), by=c("date", "STATION_NUMBER"))%>% 
  na.omit() %>% group_by(OBJECTID, STATION_NUMBER, STATION_NAME,dist_error, dist.m) %>% nest() %>% ungroup()
combo.all = NULL
for (y in 1:nrow(nest.sill)){
  df = nest.sill$data[[y]]
  obj_id = nest.sill$OBJECTID[[y]]
  stat_id = nest.sill$STATION_NUMBER[[y]]
  stat_nam = nest.sill$STATION_NAME[[y]]
  dist_error = nest.sill$dist_error[[y]]
  dist_m = nest.sill$dist.m[[y]]
  n.obs = nrow(df)
  obs.count = df %>% group_by(.pred_class) %>% count() %>% ungroup() %>% 
    mutate(all.obs = n.obs,
           pct = n/n.obs)
  if(isTRUE(obs.count$pct[obs.count$.pred_class==0]>=0.95) & nrow(df[df$.pred_class==0,])>=5  ){
    class = "always 0"
    combo = cbind.data.frame(obj_id, class, stat_id, stat_nam, dist_m, dist_error, 
                           num.0=NA, mean.0=NA, sd.0=NA, min.0=NA, max.0=NA, 
                           num.2=NA, mean.2 = NA, sd.2 =NA, min.2 = NA, max.2 = NA, 
                           pval=NA)
    combo.all = rbind.data.frame(combo.all, combo)
    next
  }
  if(isTRUE(obs.count$pct[obs.count$.pred_class==2]>=0.95)& nrow(df[df$.pred_class==2,])>=5){
    class = "always 2"
    combo = cbind.data.frame(obj_id, class, stat_id, stat_nam, dist_m, dist_error, 
                           num.0=NA, mean.0=NA, sd.0=NA, min.0=NA, max.0=NA, 
                           num.2=NA, mean.2 = NA, sd.2 =NA, min.2 = NA, max.2 = NA, 
                           pval=NA)
    combo.all = rbind.data.frame(combo.all, combo)
    next
  }
  if(isTRUE(nrow(df[df$.pred_class==2,])<5) | isTRUE(nrow(df[df$.pred_class==0,])<5)){
    next
  }
  ttest = t.test(df[df$.pred_class == 0,]$Value.0, df[df$.pred_class == 2,]$Value.0 )
  pval = ttest$p.value
  if(pval>0.05){
    class = "no discharge relationship"
    combo = cbind.data.frame(obj_id, class, stat_id, stat_nam, dist_m, dist_error, 
                           num.0=NA, mean.0=NA, sd.0=NA, min.0=NA, max.0=NA, 
                           num.2=NA, mean.2 = NA, sd.2 =NA, min.2 = NA, max.2 = NA, 
                           pval=NA)
    combo.all = rbind.data.frame(combo.all, combo)
    
    next}
  num.0 = df[df$.pred_class == 0,] %>% nrow()
  num.2 = df[df$.pred_class == 2,] %>% nrow()
  mean.0 = mean(df[df$.pred_class==0,]$Value.0)
  sd.0 = sd(df[df$.pred_class==0,]$Value.0)
  min.0 = min(df[df$.pred_class==0,]$Value.0)
  max.0 = max(df[df$.pred_class==0,]$Value.0)
  mean.2 = mean(df[df$.pred_class==2,]$Value.0)
  sd.2 = sd(df[df$.pred_class==2,]$Value.0)
  min.2 = min(df[df$.pred_class==2,]$Value.0)
  max.2 = max(df[df$.pred_class==2,]$Value.0)
  class = "discharge dependant"
  combo = cbind.data.frame(obj_id, class, stat_id, stat_nam, dist_m, dist_error, 
                           num.0, mean.0, sd.0, min.0, max.0, 
                           num.2, mean.2, sd.2, min.2, max.2, 
                           pval)
  combo.all = rbind.data.frame(combo.all, combo)
}

# calculate ranges for sills, including lakes that were in 50km of many stations
### if two stations produced two classes for a lake, take a peak at what is going on. keep the class from the station that is closest to the lake
diffclass.diffstat = combo.all %>% group_by(obj_id, class) %>% count() %>% ungroup() %>% group_by(obj_id) %>% count() %>% ungroup()%>% 
  rename(numclasses = n)
keep.obs = combo.all %>% left_join(diffclass.diffstat, by="obj_id") %>% as_tibble() %>% filter(numclasses>1) %>% arrange(obj_id, dist_m) %>% 
  group_by(obj_id) %>% filter(row_number()==1) %>% select(obj_id, stat_id, numclasses) %>% rename(keepIDs = numclasses)

combo.prep = combo.all %>% left_join(diffclass.diffstat, by="obj_id") %>% left_join(keep.obs, by=c("obj_id", "stat_id")) %>% as_tibble() %>% 
  filter(numclasses == 1 | (numclasses >1 & !is.na(keepIDs)))
  

combo.update = combo.prep %>% 
  mutate(xmin = case_when(
                       max.0>min.2 ~ min.2-as.numeric(dist_error), 
                       max.0<min.2 ~ max.0-as.numeric(dist_error)
                     ),
         xmax = case_when(
           max.0>min.2 ~ max.0+as.numeric(dist_error),
           max.0<min.2 ~ min.2 + as.numeric(dist_error)
                    ),
         mid.sill = (xmin+xmax)/2) %>% 
  as_tibble()

# for the lakes that are in 50km of multiple stations, overlap all the sill elevation ranges
compare.sills = combo.update%>% as_tibble() %>% 
  filter(!is.na(mid.sill)) %>% group_by(obj_id) %>% count() %>% 
  filter(n>1) %>% left_join(combo.update  %>% filter(!is.na(mid.sill)), by="obj_id" ) %>% 
  select(obj_id, n, stat_id, dist_m, mid.sill, xmin, xmax) %>% group_by(obj_id, n) %>% nest() 
return_all = NULL
for (b in 1:nrow(compare.sills)){
  sill.df = compare.sills$data[[b]] %>% arrange(dist_m) %>% as.data.table()
  obj_id = compare.sills$obj_id[[b]]
  n = compare.sills$n[[b]]
  results = cbind.data.frame(obj_id, n, sill.df[, .(max(xmin), min(xmax))])
  return_all = rbind.data.frame(return_all, results)
}
sill.compare = combo.update %>% left_join(return_all, by="obj_id") %>% group_by(obj_id) %>% arrange(dist_m) %>% filter(row_number()==1) %>% 
  ungroup() %>% 
  mutate(diff = V2-V1,
         fin.min = case_when(
           !is.na(V1) & diff>0 ~ V1,
           is.na(V1) ~ xmin,
         ),
         fin.max = case_when(
           !is.na(V2) & diff>0 ~ V2,
           is.na(V2) ~ xmax,
         )) %>% select(obj_id, class,fin.min, fin.max) %>% 
  mutate(fin.range = fin.max-fin.min,
         fin.sill = (fin.max+fin.min)/2) %>% 
  left_join(lakes.sf %>% 
                              rename(obj_id=OBJECTID) %>% select(obj_id, geometry),
                          by=c("obj_id")) %>% st_as_sf() %>% st_transform(crs.analyze)




p1=ggplot()+
  geom_sf(data=sill.compare %>% filter(fin.range<1 & !is.na(fin.sill)),
          aes(fill=fin.sill), color=NA)+
  scale_fill_viridis_c(option="inferno")+
  theme_bw()+
  geom_sf(data=mack.basin.large, color="grey70", size=0.5)+
  theme(panel.grid = element_blank(),
        panel.background = element_rect(fill="grey90"),
        legend.position="bottom",
        axis.text.y = element_text(size=12),
        axis.text.x = element_text(size=12, angle=45, hjust=1),
        legend.text = element_text(size=12, angle=45, hjust=1),
        plot.background = element_blank(),
        legend.background = element_blank(),
        legend.title = element_text(size=12, face="bold"))+
  labs(fill="average\nfunctional\nsill elevation\n(m)")+
   guides(fill = guide_colorbar(barwidth = 8, barheight = 0.5))

p2 = ggplot()+
  geom_sf(data=sill.compare %>% filter(fin.range<1 & !is.na(fin.sill)),
          aes(fill=fin.range), color=NA)+
  scale_fill_viridis_c(option="viridis", limits = c(0,1))+
  geom_sf(data=mack.basin.large, color="grey70", size=0.5)+
  theme_bw()+
  theme(
    panel.grid = element_blank(),
    axis.text.y = element_text(size=12),
    axis.text.x = element_text(size=12, angle=45, hjust=1),
    panel.background = element_rect(fill="grey90"), 
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.position = "bottom",
    legend.text = element_text(size=12, angle=45, hjust=1),
    legend.title = element_text(size=12, face="bold"))+
  labs(fill="width of uncert-\nainty bounds\n(m)")+
   guides(fill = guide_colorbar(barwidth = 8, barheight = 0.5))


p3=ggplot()+
  geom_sf(data=sill.compare %>% filter(fin.range> 1 | is.na(fin.sill)) %>% 
            mutate(class = ifelse(class=="discharge dependant", 
                                  "high uncertainty\ndischarge dependant", 
                                  class)) %>% 
            mutate(class = ifelse(class=="no discharge relationship", 
                                  "no significant\ndischarge relationship", class)),
          aes(fill=class), color=NA)+
  #scale_fill_viridis_c(option="viridis", limits = c(0,1))+
  geom_sf(data=mack.basin.large, color="grey70", size=0.5)+
  theme_bw()+
  theme(
    panel.grid = element_blank(),
    legend.background = element_blank(),
    axis.text.y = element_text(size=12),
    axis.text.x = element_text(size=12, angle=45, hjust=1),
    panel.background = element_rect(fill="grey90"), 
    plot.background = element_blank(),
    legend.position = "bottom",
    legend.text = element_text(size=12),
    legend.title = element_blank())+
   guides(fill=guide_legend(nrow=4,byrow=TRUE))





p1+p2+p3
setwd(images.wd)
ggsave("sills.png", width=10.5, height=8.5, units = "in")


sill.compare %>% as_tibble() %>% select(-geometry) %>% 
  mutate(final.class = case_when(
    class=="always 0" ~ "always 0",
    class=="always 2" ~ "always 2",
    class=="discharge dependant" & fin.range<1 ~ "discharge dependant (<1m uncert)",
    class=="discharge dependant" & (fin.range>1 | is.na(fin.range)) ~ "discharge dependant (>1m uncert or unable to calculate sill)",
    class=="no discharge relationship" ~ "no discharge relationship",
  )) %>% group_by(final.class) %>% count()




# look at sills from lesack/marsh lakes
sill.compare %>% mutate(lw.name = case_when(obj_id== 3244329 ~ "L80",
    obj_id== 3305102 ~ "L129",
    obj_id==3243679 ~ "L527a",
    obj_id == 3246340 ~ "L278")) %>% filter(!is.na(lw.name))


# combo.all %>% mutate(guess.sill = ((mean.0+mean.2)/2),
#                      se = 0.5 * sqrt((sd.0/num.0)^2 + (sd.2/num.2)^2 ),
#                      xmin = case_when(
#                        max.0>min.2 ~ min.2-as.numeric(dist_error), 
#                        max.0<min.2 ~ max.0-as.numeric(dist_error)
#                      ),
#                      xmax = case_when(
#                        max.0>min.2 ~ max.0+as.numeric(dist_error),
#                        max.0<min.2 ~ min.2 + as.numeric(dist_error)
#                      )) %>% 
#   rename(STATION_NUMBER = stat_id) %>% 
#   ggplot()+
#   geom_rect(aes(xmin = xmin, xmax = xmax, ymin=0, ymax=2))+
#     geom_point(data = nest.sill %>% 
#                  unnest(data), aes(x=Value.0, y=.pred_class), alpha=0.5 )+
#   geom_text(aes(x=13, y=1.5), label = as.numeric(dist_error))+
#     facet_wrap(~STATION_NUMBER, nrow=1, scales="free_x")+ylim(0,2)
# 
# 
# flow.prep = flow.df.all %>% 
#    select(STATION_NUMBER, Date, Value) %>% 
#    spread(STATION_NUMBER, Value) %>% 
#    na.omit() %>% select(-Date) %>% colMeans() 
# 
# col.names = flow.df.all %>% 
#    select(STATION_NUMBER, Date, Value) %>% 
#    spread(STATION_NUMBER, Value) %>% 
#    na.omit() %>% select(-Date) %>% colnames()
# 
# 
# flow.prep2 = cbind.data.frame(flow.prep %>% as_tibble(), col.names) %>% 
#   rename(STATION_NUMBER = col.names) %>% 
#   left_join(flow.location.df %>% 
#               select(STATION_NAME, STATION_NUMBER, geometry), by="STATION_NUMBER") %>% 
#   st_as_sf()
# 
# dist.matrix = st_distance(flow.prep2)
# dimnames(dist.matrix) = list(col.names, col.names)
# dist.df = t(combn(colnames(dist.matrix), 2))
# dist.df = data.frame(dist.df, dist = dist.matrix[dist.df])
# 
# dif.matrix = dist(flow.prep2$value, diag=T, upper=T) %>% as.matrix()
# dimnames(dif.matrix) = list(col.names, col.names)
# dif.df = t(combn(colnames(dif.matrix), 2))
# dif.df = data.frame(dif.df, dist = dif.matrix[dif.df]) %>% rename(dif = dist)
# 
# 
# dist.dif.df = dist.df %>% left_join(dif.df, by=c("X1", "X2")) %>% as_tibble()
# 
# dist.dif.df %>% 
#   filter(X1 != "10MC002" &  X1 != "10LC014") %>% 
#   filter(X2 != "10MC002" &  X2 != "10LC014") %>% 
#   ggplot()+geom_point(aes(x=dist, y=dif, color=X1))+theme_classic()+
#   geom_smooth(aes(x=dist, y= dif), method="lm")
# 
# dist.dif.df.filt = dist.dif.df %>% 
#   filter(X1 != "10MC002" &  X1 != "10LC014") %>% 
#   filter(X2 != "10MC002" &  X2 != "10LC014")
# 
# dist.dif.mod = lm(dif ~ as.numeric(dist), data =dist.dif.df.filt )
# mod.int = dist.dif.mod$coefficients[[1]]
# mod.slope = dist.dif.mod$coefficients[[2]]
# 






# ### Do the exact same correlation analysis, except with individual lakes
# correl.prep.all.lake = import.data %>% 
#   select(.pred_class, OBJECTID, date) %>% 
#   mutate(.pred_class = as.numeric(as.character(.pred_class))) %>% 
#   mutate(date.7 = date-days(7),
#          date.14 = date-days(14)) %>% 
#   left_join(flow.df %>% rename(date=Date, Value.0 = Value) %>% 
#               select(STATION_NUMBER, Value.0, date), by="date") %>% 
#   left_join(flow.df %>% rename(date.7=Date, Value.7 = Value) %>% 
#               select(STATION_NUMBER, Value.7, date.7), by=c("date.7", "STATION_NUMBER")) %>% 
#   left_join(flow.df %>% rename(date.14=Date, Value.14 = Value) %>% 
#               select(STATION_NUMBER, Value.14, date.14), by=c("date.14", "STATION_NUMBER")) %>% 
#   select(-date.7, -date.14) %>% 
#   group_by(OBJECTID, STATION_NUMBER) %>% nest() %>% ungroup() %>% filter(!is.na(STATION_NUMBER))
# 
# 
# row.combo.all.lakes=NULL
# for (i in 1:length(correl.prep.all.lake$OBJECTID)){
#   dat=correl.prep.all.lake$data[[i]] %>% na.omit()
#   OBJECTID = correl.prep.all.lake$OBJECTID[[i]]
#   stat = correl.prep.all.lake$STATION_NUMBER[[i]]
#   len = length(dat$date)
#   if(length(dat$date)<20){next}
#   cor.0 = cor.test(dat$Value.0, dat$.pred_class, method = "kendall", alternative="greater")
#   coeff.0 = cor.0$estimate[[1]]
#   pval.0 = cor.0$p.value
#   cor.7 = cor.test(dat$Value.7, dat$.pred_class, method = "kendall",alternative="greater")
#   coeff.7 = cor.7$estimate[[1]]
#   pval.7 = cor.7$p.value
#   cor.14 = cor.test(dat$Value.14, dat$.pred_class, method = "kendall", alternative="greater")
#   coeff.14 = cor.14$estimate[[1]]
#   pval.14 = cor.14$p.value
#   col.combo = cbind.data.frame(OBJECTID, stat, len, coeff.0, pval.0, coeff.7, pval.7, coeff.14, pval.14)
#   row.combo.all.lakes = rbind.data.frame(row.combo.all.lakes, col.combo)
# }
# 
# prep.all.lakes = row.combo.all.lakes %>% as_tibble() %>% select(-len) %>% 
#   rename(STATION_NUMBER=stat) %>% 
#   rownames_to_column() %>% 
#   gather(column, value, -rowname, -OBJECTID, -STATION_NUMBER, -pval.0, -pval.14, -pval.7) %>% 
#   mutate(pval = case_when(
#     column=="coeff.0"~ pval.0,
#     column=="coeff.7"~ pval.7,
#     column=="coeff.14"~ pval.14
#   )) %>% select(-pval.0, -pval.14,-pval.7, -rowname) %>% 
#   mutate(lag.days = as.numeric(sub(".*\\.","", column))) %>% 
#   left_join(lakes.sf %>% select(OBJECTID, geometry), by="OBJECTID") %>% st_as_sf() %>% 
#   mutate(lag.text = case_when(
#   lag.days == 0 ~ "0 days",
#   lag.days == 7 ~ "7 days",
#   lag.days == 14 ~ "14 days")) %>% 
#   left_join(flow.location.df %>% as_tibble() %>% 
#               select(STATION_NUMBER, STATION_NAME), by="STATION_NUMBER") %>% st_transform(crs.plot)
# 
# prep.all.sum.lakes = prep.all.lakes %>% filter(pval<0.05 & value>=0) %>% as_tibble() %>% select(-geometry)%>% group_by(OBJECTID, STATION_NAME, STATION_NUMBER) %>% 
#   summarise(max.value = max(value),
#             pval = pval[value==max.value]) %>%
#   select(OBJECTID, STATION_NAME, STATION_NUMBER, max.value, pval) %>% ungroup() %>% 
#   mutate(lag.text="maximum positive\ncorrelation") %>% rename(value=max.value) %>% 
#   left_join(lakes.sf %>% select(OBJECTID, geometry), by="OBJECTID") %>% st_as_sf() %>% st_transform(crs.plot)
# 
# prep.all.sum.all.lakes = prep.all.lakes %>% filter(pval>0.05 | value<0)%>% as_tibble() %>% select(-geometry) %>% group_by(OBJECTID, STATION_NAME, STATION_NUMBER) %>% 
#   summarise(max.value = max(value),
#             pval = pval[value==max.value]) %>%
#   select(OBJECTID, STATION_NAME, STATION_NUMBER, max.value, pval) %>% ungroup() %>% 
#   mutate(lag.text="maximum positive\ncorrelation") %>% rename(value=max.value)%>% 
#   left_join(lakes.sf %>% select(OBJECTID, geometry), by="OBJECTID") %>% st_as_sf() %>% st_transform(crs.plot)
# 
# prep.all.combo.lakes = rbind.data.frame(prep.all.lakes %>% select(-column, -lag.days), prep.all.sum.lakes, prep.all.sum.all.lakes)
# 
# prep.all.combo.lakes$lag.text = factor(prep.all.combo.lakes$lag.text, 
#                            levels=c("0 days", "7 days", "14 days", "maximum positive\ncorrelation"))
# 
# 
# library(colorspace)
# ggplot()+
#   geom_sf(data = prep.all.combo.lakes %>% filter(STATION_NUMBER %in% good.stations),
#           fill="grey10", color=NA)+
#   geom_sf(data=prep.all.combo.lakes %>% filter(STATION_NUMBER %in% good.stations) %>% 
#             filter(pval<=0.05 & value>=0), 
#           aes(fill=value), color=NA)+
#    geom_sf(data=mack.basin.large, color="grey40", size=0.2 )+theme_bw()+
#   geom_sf(data=flow.location.df %>% 
#             filter(STATION_NUMBER %in% good.stations), color="black", size=5)+
#     facet_grid(vars(STATION_NAME), vars(lag.text),
#                labeller = label_wrap_gen(width = 25, multi_line = TRUE))+
#   scale_fill_continuous_sequential(
#                        guide=guide_colorbar(barheight = 20),
#                        palette="PinkYl", limits=c(0.25,0.5), breaks = c(0.25, 0.3, 0.35, 0.4, 0.45, 0.5),
#                        labels =c("<0.25", "0.3", "0.35", "0.4", "0.45", ">0.5"),oob=scales::squish)+
#   theme(axis.text.x = element_text(angle=45, hjust=1, size=12))+
#    theme(panel.background = element_rect(fill="grey55"),
#         panel.grid = element_line(color="grey50"),
#           title = element_text(size=12, face="bold"),
#           axis.text.y = element_text(size=12),
#           legend.title = element_text(size=14, face="bold"),
#           legend.text=element_text(size=14),
#         strip.text = element_text(size=12, face="bold")
#     )+labs(fill="Kendall's\nTau")
# setwd(images.wd)
# ggsave("dischargeLakeCorr.png", width=12, height=12, units="in")
# 





## Create the gif
setwd(int.wd)
all.classified.filter=read_rds(paste0("final.class_", stage3.date, ".Rdata"))

setwd(gif.wd)
prep.gif.data = all.classified.filter %>% filter(month==6) %>% 
  st_as_sf() %>% st_transform(crs = crs.plot) %>% 
  group_by(year) %>% nest() %>% ungroup()

### Loop through each year
for (z in 1: length(prep.gif.data$year)){
  dat = prep.gif.data$data[[z]] %>% 
    mutate(.pred_class=as.numeric(as.character(.pred_class))) %>% as_tibble() %>% select(-geometry) %>% 
    group_by(OBJECTID) %>% 
    summarise(mean.class = mean(.pred_class, na.rm=T)) %>% left_join(lakes.sf, by="OBJECTID")%>% st_as_sf() %>% st_transform(crs=crs.plot)

  year.main = prep.gif.data$year[[z]]
  
  #### Create plot 1 (map of June connectivity)
  scale = scale_fill_gradientn(colours = c("#619CFF","#00BA38","#F8766D"))
  p1 = ggplot(data=dat)+
   geom_sf(aes(fill=mean.class), color=NA)+
    theme_bw()+scale+
    annotation_scale(text_cex = 0.9)+
    geom_sf(data=mack.basin.large, color="grey65")+
    geom_sf(data=study.area.large %>% st_transform(crs.plot), color=NA, fill=NA)+
    #geom_sf(data=study.area, color=NA, fill=NA)+
    geom_sf(data=flow.location.df %>% filter(STATION_NUMBER %in% c("10MC023", "10LC014")), 
            aes(color=STATION_NAME), size=5)+
    scale_colour_manual(guide="none",     values=c("#000000", "#ABA9A9"))+
    theme(axis.text.x = element_text(angle=45, hjust=1, size=12),
          strip.text = element_text(size=14, face="bold"),
          axis.text.y = element_text(size=12),
         # legend.title = element_text(size=16, face="bold"),
          legend.text=element_text(size=16, face="bold"),
          title=element_text(size=18, face="bold"),
          legend.position="bottom", legend.direction="horizontal",
          legend.key.size=unit(1, "cm"),
          legend.title=element_blank(),
         legend.box.spacing = unit(0, "pt"),
         legend.margin=margin(0,0,0,0))+labs(fill="Class")+
    guides(fill=guide_legend(label.position="top",label.vjust = -8, title.vjust = 0.2))
  #### create plot 2 (date of peak water level)
  
  p2= max.doy.df.sb %>%filter(stat %in% c("10MC023", "10LC014")) %>% left_join(flow.location.df %>% as_tibble() %>% 
                                                                                 select(STATION_NUMBER, STATION_NAME) %>% 
                                                                                 rename(stat=STATION_NUMBER), by="stat") %>% 
    as_tibble() %>% mutate(facet=str_wrap("date of peak discharge or water level", 20)) %>% 
    ggplot()+geom_line(aes(x=yr, y=max.first.doy, color=STATION_NAME))+
    geom_point(aes(x=yr, y=max.first.doy,color=STATION_NAME), size=1.5)+theme_bw()+
    geom_point(data=max.doy.df.sb %>%filter(stat %in% c("10MC023", "10LC014")) %>% left_join(flow.location.df %>% as_tibble() %>% 
                                                                                 select(STATION_NUMBER, STATION_NAME) %>% rename(stat=STATION_NUMBER), by="stat") %>% 
                 filter(yr==year.main), aes(color=STATION_NAME, x=yr, y=max.first.doy), size=5)+
    #ylab(str_wrap("date of peak discharge or water level", width=30))+
    xlab("")+facet_wrap(~facet, strip.position="left")+
    #geom_smooth(method=lm, aes(x=yr, y=max.first.value), se=F)+
    theme(axis.text.x = element_text( size=12),
          axis.title.y=element_blank(),
          strip.text = element_text(size=14, face="bold"),
          title = element_text(size=14, face="bold"),
          axis.text.y = element_text(size=12),
          legend.title = element_blank(),
          legend.text=element_text(size=14),
          legend.position = "bottom", strip.placement="outside", strip.background = element_blank())+
    scale_colour_manual(labels = function(x) str_wrap(x, width = 30),
                         values=c("#000000", "#ABA9A9") )+
    scale_y_continuous(breaks = c(121,135,152, 166, 182), labels = c("May 1st", "May 15th", 
                                                                     "June 1st", "June 15th", "July 1st"))+
    labs(colour="WSC Station Name")+xlim(2000,2020)+guides(color=guide_legend(nrow=2))
  #### Create plot 3 (height of peak water level)
  p3= max.doy.df.sb %>%filter(stat %in% c("10MC023", "10LC014")) %>% left_join(flow.location.df %>% as_tibble() %>% 
                                                                                 select(STATION_NUMBER, STATION_NAME) %>% rename(stat=STATION_NUMBER), by="stat") %>% 
    as_tibble() %>%  
    ggplot()+geom_line(aes(x=yr, y=max.first.value, color=STATION_NAME))+
    geom_point(aes(x=yr, y=max.first.value,color=STATION_NAME), size=1.5)+theme_bw()+
     geom_point(data=max.doy.df.sb %>%filter(stat %in% c("10MC023", "10LC014")) %>% left_join(flow.location.df %>% as_tibble() %>% 
                                                                                 select(STATION_NUMBER, STATION_NAME) %>% rename(stat=STATION_NUMBER), by="stat") %>% 
                 filter(yr==year.main), aes(color=STATION_NAME, x=yr, y=max.first.value), size=5)+
    ylab(str_wrap("peak water level (m) / discharge (cms)", width=10))+
    #geom_smooth(method=lm, aes(x=yr, y=max.first.value), se=F)+
    theme(axis.text.x = element_text( size=12),
          strip.text = element_text(size=12, face="bold"),
          title = element_text(size=14, face="bold"),
          axis.text.y = element_text(size=12),
          axis.title.x = element_blank(),
          axis.title.y=element_blank(),
          legend.title = element_blank(),
          legend.text=element_text(size=14),
          legend.position="bottom",
          strip.placement = "outside",
          strip.background = element_blank())+
    scale_colour_manual(labels = function(x) str_wrap(x, width = 30),
                         values=c("#000000", "#ABA9A9") )+
    labs(colour="WSC Station Name")+xlim(2000,2020)+facet_wrap(~STATION_NAME, ncol=1, scales="free_y", strip.position="left", 
                                                               labeller = as_labeller(c("MACKENZIE RIVER (NAPOIAK CHANNEL) ABOVE SHALLOW BAY" = str_wrap("peak water level (m)", 10),"MACKENZIE RIVER AT ARCTIC RED RIVER"=str_wrap("peak discharge (cms)", 10))))+guides(color=guide_legend(nrow=2))

  #### Combine plots 2 and 3 into one plot 
  p23 = p2/p3+plot_layout(guides="collect") &theme(legend.position="bottom")
  #### Combine the p2/3 plot with the map
  gif.plot = p1+p23+plot_layout(widths=c(4, 5)) & 
    theme(legend.position="bottom", plot.title = element_text(size=30,face="bold", hjust=0.5)) &
    plot_annotation(title=paste("June ",year.main))
  #### Save the gif to your file
  ggsave(plot=gif.plot,filename= paste0("year", year.main, ".png") ,width=14, height=9.5, units = "in") 
}


### List all the files in the gif and combine them into a stacked image
setwd(gif.wd)
files=list.files(pattern="*.png")
images <- map(files, image_read)
images <- image_join(images)
### Animate the stacked image
gif = image_animate(images, fps = 1, dispose = "previous")

## save as a gif
setwd(images.wd)
image_write(gif, paste0("gif_", stage4.date, ".gif"))



```


#Resuspension
```{r}
setwd(int.wd)
import.data =  read_rds(paste0("final.class_", stage3.date, ".Rdata")) 

import.data %>% left_join(islands.join %>% as_tibble() %>% select(fid, OBJECTID), by="OBJECTID") %>% 
  mutate(.pred_class = as.numeric(as.character(.pred_class))) %>% 
  filter(fid=="1126" & .pred_class == 2) %>% 
  mutate(fxd_ndx=row_number()) %>% st_write("fid1126class2.shp")


import.data %>% filter(med_R_ratio_m>1.3) %>% 
  group_by(OBJECTID) %>% mutate(group_id=row_number()) %>% filter(group_id==1) %>% ungroup() %>% 
  mutate(fxd_ndx=row_number()) %>% st_write("hR_gt1_3.shp")

```



# Analyze spatial patterns in connectivity
```{r}
## Import classification results & group by year and month
setwd(int.wd)
results.import = read_rds(paste0("final.class_", stage3.date, ".Rdata")) %>% lazy_dt() 

results.summary.subgroups = results.import %>% dplyr::select(.pred_class, OBJECTID, year, month) %>% 
  mutate(yeargroup = case_when(
    year>=1984 & year<=1999 ~ "1984-1990",
    year>=2000 & year<=2022 ~ "2000-2022"
  )) %>% filter(!is.na(yeargroup)) %>% 
  group_by(OBJECTID, month, year, yeargroup)%>% 
  summarise(class.mean = mean(as.numeric(as.character(.pred_class)), na.rm=T),
            count=n()) %>% ungroup()

results.summary.all = results.import %>% dplyr::select(.pred_class, OBJECTID, year, month) %>% 
  mutate(yeargroup = "all") %>% 
  group_by(OBJECTID, month, year, yeargroup)%>% 
  summarise(class.mean = mean(as.numeric(as.character(.pred_class)), na.rm=T),
            count=n()) %>% ungroup()


results.summary = rbind.data.frame(results.summary.subgroups %>% as_tibble(), 
                                   results.summary.all %>% as_tibble())
### rivers for plotting
crs.plot="+proj=tcea +lon_0=-134.3847656 +datum=WGS84 +units=m +no_defs"
crs.analyze = "EPSG:32608"
setwd(shapeFiles.filePath)
study.area.large=cbind.data.frame(lon=c(-136.80, -136.80, -133.47, -133.47), 
                 lat=c(67.25, 69.55, 69.55, 67.46)) %>% 
  st_as_sf(coords=c("lon", "lat")) %>% st_set_crs(4326) %>% st_bbox() %>% st_as_sfc() %>% 
  st_transform(crs = crs.plot)
mack.basin.large = st_read(import.sword) %>% 
  st_transform(crs = crs.plot) %>% 
  st_intersection(study.area.large) %>% dplyr::filter(width>90)
## single lake trends
## Get ids of lakes with at least 10 years of classifications in each month




good.ids = results.summary %>% group_by(OBJECTID, month, yeargroup) %>%count() %>% ungroup() %>% 
  filter(n>=10) 

best.ids = good.ids %>% group_by(OBJECTID, month) %>% count() %>% ungroup() %>% filter(n==3)


nested.data = results.summary %>%
  left_join(best.ids, by=c("OBJECTID", "month")) %>% 
  dplyr::filter(!is.na(n)) %>% 
  group_by(OBJECTID, month, yeargroup) %>% nest() %>% ungroup() %>% as_tibble()
## for each lake, calculate the trend (tau) and pvalue
row.combo=NULL
for (i in 1:nrow(nested.data)){
  dat = nested.data$data[[i]] %>% arrange(year)
  OBJECTID = nested.data$OBJECTID[[i]]
  month = nested.data$month[[i]]
  yeargroup = nested.data$yeargroup[[i]]
  n.obs = nrow(dat)
  obs.count = dat %>% group_by(class.mean) %>% count() %>% ungroup() %>% 
    mutate(all.obs = n.obs,
           pct = n/n.obs)
  if(isTRUE(obs.count$pct[obs.count$class.mean<=0.66]>=0.95)){
    class = "always less than 0.66"
    col.combo = cbind.data.frame(OBJECTID, month,yeargroup,class, pval=NA, S=NA, tau=NA)
    row.combo=rbind.data.frame(row.combo, col.combo)
  } else if(isTRUE(obs.count$pct[obs.count$class.mean>0.66 |obs.count$class.mean<=1.33]>=0.95)){
    class = "always 0.66-1.33"
    col.combo = cbind.data.frame(OBJECTID, month,yeargroup,class, pval=NA, S=NA, tau=NA)
    row.combo=rbind.data.frame(row.combo, col.combo)
  }else if(isTRUE(obs.count$pct[obs.count$class.mean>1.33]>=0.95)){
    class = "always >1.33"
    col.combo = cbind.data.frame(OBJECTID, month,yeargroup,class, pval=NA, S=NA, tau=NA)
    row.combo=rbind.data.frame(row.combo, col.combo)
  } else {
    class = "trendtest"
    test.obj=MannKendall(dat$class.mean)
    S=test.obj$S[[1]]
    tau = test.obj$tau
    pval = test.obj$sl
    col.combo = cbind.data.frame(OBJECTID, month,yeargroup, class, pval, S, tau)
    row.combo=rbind.data.frame(row.combo, col.combo)
  }
  
}
## Format trend data results
trend.data=row.combo %>% as_tibble()%>% 
  mutate(trend = case_when(
    tau>0 & pval < 0.05 ~ "increasing connectivity trend",
    tau<0 & pval < 0.05~ "decreasing connectivity trend",
    pval>0.05 ~ "no monotonic trend")) %>% 
  left_join(lakes.sf, by="OBJECTID") %>% st_as_sf() %>% 
   st_transform(crs = crs.plot)


row.combo %>% as_tibble()%>% 
  mutate(trend = case_when(
    tau>0 & pval < 0.05 ~ "increasing connectivity trend",
    tau<0 & pval < 0.05~ "decreasing connectivity trend",
    pval>0.05 ~ "no monotonic trend",
    is.na(tau) & class == "always less than 0.66" ~ "always less than 0.66",
    is.na(tau) & class == "always 0.66-1.33" ~ "always 0.66-1.33",
    is.na(tau) & class == "always >1.33" ~ "always >1.33")) %>% 
  group_by(month, yeargroup, trend) %>% count()

## Select colors for trend plot  

## Create trend map

sig.trend = ggplot(data=trend.data %>% filter(!is.na(tau)) %>% 
                     filter(month==6 & pval <= 0.05))+
  geom_sf(data=mack.basin.large, color="grey75", size=0.01)+
  geom_sf(aes(fill=tau), color=NA)+
   scale_fill_gradientn(
   colors=c("blue","grey85","red"),
   values=scales::rescale(c(-1,0,1)),
   limits=c(-1,1)
  )+
  theme_bw()+
  annotation_scale(text_cex = 0.9)+
  theme(axis.text.x = element_text(angle=45, hjust=1, size=12),
        strip.text = element_text(size=14, face="bold"),
        title = element_text(size=14, face="bold"),
        axis.text.y = element_text(size=12),
        legend.title = element_text(size=12),
        legend.text=element_text(size=10),
        legend.position = "bottom",
        legend.justification = "center")+labs(fill="Trend")+
  #guides(fill=guide_legend(nrow=1, legend.justification="center"))+
  facet_wrap(~yeargroup)+labs(fill="Kendall's Tau")+
  ggtitle("Lakes with significant trends (p<0.05)")

some.trend = ggplot(data=trend.data %>% filter(!is.na(tau)) %>% 
                     filter(month==6))+
  geom_sf(data=mack.basin.large, color="grey75", size=0.01)+
  geom_sf(aes(fill=tau), color=NA)+
   scale_fill_gradientn(
   colors=c("blue","grey85","red"),
   values=scales::rescale(c(-1,0,1)),
   limits=c(-1,1)
  )+
  theme_bw()+
  annotation_scale(text_cex = 0.9)+
  theme(axis.text.x = element_text(angle=45, hjust=1, size=12),
        strip.text = element_text(size=14, face="bold"),
        title = element_text(size=14, face="bold"),
        axis.text.y = element_text(size=12),
        legend.title = element_text(size=12),
        legend.text=element_text(size=10),
        legend.position = "bottom",
        legend.justification = "center")+labs(fill="Trend")+
  #guides(fill=guide_legend(nrow=1, legend.justification="center"))+
  facet_wrap(~yeargroup)+labs(fill="Kendall's Tau")+
  ggtitle("All lakes with variable connectivity")

low.var = ggplot(data=trend.data %>% filter(is.na(tau)) %>% 
                     filter(month==6))+
  geom_sf(data=mack.basin.large, color="grey75", size=0.01)+
  geom_sf(aes(fill=class), color=NA)+
  #  scale_fill_gradientn(
  #  colors=c("blue","grey85","red"),
  #  values=scales::rescale(c(-1,0,1)),
  #  limits=c(-1,1)
  # )+
  theme_bw()+
  annotation_scale(text_cex = 0.9)+
  theme(axis.text.x = element_text(angle=45, hjust=1, size=12),
        strip.text = element_text(size=14, face="bold"),
        title = element_text(size=14, face="bold"),
        axis.text.y = element_text(size=12),
        legend.title = element_text(size=12),
        legend.text=element_text(size=10),
        legend.position = "bottom",
        legend.justification = "center")+labs(fill="Trend")+
  #guides(fill=guide_legend(nrow=1, legend.justification="center"))+
  facet_wrap(~yeargroup)+labs(fill="Kendall's Tau")+
  ggtitle("All lakes with low variability")

sig.trend / some.trend / low.var

setwd(images.wd)
ggsave("lakeTrendAll.png", width=10, height=16, units="in")


trend.data %>% filter(pval<0.05 &trend=="increasing connectivity trend" & month==8 )

results.summary %>% filter(OBJECTID==2985638) %>% as_tibble() %>% filter(month==8) %>% 
  ggplot()+
  geom_point(aes(x=year, y=class.mean), alpha=0.5)+
  facet_wrap(~month, nrow = 4)

flow.df.all %>% lazy_dt() %>%  group_by(STATION_NUMBER, month, year) %>% summarise(mean.value = mean(Value, na.rm=T), max.value=max(Value, na.rm=T)) %>% filter(month==8) %>% as_tibble() %>% ggplot()+geom_line(aes(x=year, y=mean.value))+facet_wrap(~STATION_NUMBER, scales="free_y")+geom_point(aes(x=year, y=mean.value))+geom_point(aes(x=year, y=max.value))+geom_line(aes(x=year, y=max.value, linetype="max value"), linetype=10)

setwd(images.wd)
ggsave("trendMap.png", device="png", width = 8, height=5, units="in")




####### Regional trends
# Analyze regional trends
## Import islands
setwd(pil.wd)
mack.islands = st_read("MackenzieDeltaIslands.shp") %>% 
  dplyr::select(fid, geometry) %>% 
  st_transform(crs.plot)
mack.islands$area = units::set_units(st_area(mack.islands), km^2)

#Join lakes to islands
islands.join = lakes.sf %>% st_transform(crs.plot) %>% st_join(mack.islands) %>% filter(!is.na(fid)) 

# keep lakes with 
good.lakes = results.summary %>% group_by(OBJECTID, month, decade) %>% count() %>% 
  filter(n>10) %>% ungroup()

# get info about how many lakes each island has maximum
fid.info =results.summary %>% 
  left_join(good.lakes, by=c("OBJECTID", "month", "decade")) %>% filter(!is.na(n)) %>% 
  left_join(islands.join %>% select(fid, OBJECTID, geometry), by="OBJECTID") %>% 
  group_by(fid) %>% 
  summarise(max.ids = n_distinct(OBJECTID))%>% ungroup() 

# select only those month/years when they have at least 50% of lakes observed and regularly observe more than 10 lakes
good.fids=results.summary %>% 
  left_join(good.lakes, by=c("OBJECTID", "month", "decade")) %>% filter(!is.na(n)) %>% 
  left_join(islands.join %>% select(fid, OBJECTID, geometry), by="OBJECTID") %>% 
  group_by(fid,month, decade) %>% 
  summarise(uniqueLakes = 
              n_distinct(OBJECTID)) %>% ungroup() %>% left_join(fid.info, by="fid") %>% 
  mutate(pct.lakes = uniqueLakes/max.ids) %>% 
  filter(pct.lakes>0.5 & max.ids>20)
# actually prep the data for analysis
island.nest = results.summary %>%
  #left_join(good.lakes, by=c("OBJECTID", "month")) %>% filter(!is.na(n)) %>% 
  left_join(islands.join %>% select(fid, OBJECTID, geometry), by="OBJECTID") %>% 
  dplyr::filter(!is.na(fid)) %>% 
  left_join(good.fids, by=c("fid", "month","decade")) %>% filter(!is.na(pct.lakes)) %>%  
  arrange(OBJECTID, year, month) %>% 
  group_by(fid, month, decade) %>% nest() %>% ungroup() %>% as_tibble()



# do mann kendall!
library(EnvStats)
row.rmk = NULL
for (z in 1:length(island.nest$data)){
  print(z)
  dat = island.nest$data[[z]] %>% na.omit()
  fid = island.nest$fid[[z]]
  month = island.nest$month[[z]]
  decade = island.nest$decade[[z]]
  yrs = length(unique(dat$year))
  #if(yrs <10){next}
  rmk = kendallSeasonalTrendTest(data=dat, y = class.mean~OBJECTID+year)
  tau = rmk$estimate[[1]]
  pval.cs = rmk$p.value[[1]]
  pval.z = rmk$p.value[[2]]
  alt.hyp = rmk$alternative
  col.rmk = cbind.data.frame(fid, month, decade, tau, pval.cs, pval.z)
  row.rmk = rbind.data.frame(row.rmk, col.rmk)
}
## summarize findings
island.trends = row.rmk %>% left_join(mack.islands, by="fid") %>% 
  st_as_sf() %>% mutate(trend =
                          case_when(pval.z>0.05 ~ "no trend",
                                    pval.cs <0.05 ~"inconsistent trend",
                                    pval.z<=0.05 & tau<0 & 
                                      (pval.cs>0.05 | is.na(pval.cs)) ~ "decreasing trend",
                                    pval.z<=0.05 & tau>0 & 
                                      (pval.cs > 0.05 | is.na (pval.cs))~ "increasing trend"),
                        monthWord = case_when(month==6 ~"June",month==7 ~"July",
                                              month==8~"August",
                                              month==9~"September")) %>% 
  filter(month!=5) # remove may bcs not enough data

## Add factors to months and trends for ease of plotting
island.trends$monthWord = factor(island.trends$monthWord, 
                                 levels=c("June", "July", "August", "September"))
island.trends$trend = factor(island.trends$trend, 
                             levels = c("increasing trend", "no trend", 
                                        "decreasing trend", "inconsistent trend"))
scale_params <- tibble::tibble(
   monthWord = factor("June", levels = c("June", "July", "August", "September")),
   Key ="trend"
  )

ggplot()+
  #geom_sf(data=mack.basin.large, color="grey50", size=0.1)+
  geom_sf(data=island.trends %>% filter(decade=="2000-2022"), aes(fill=trend), color=NA)+
  annotation_scale(text_cex=1.2, data=scale_params)+
  theme_bw()+
  scale_fill_manual(values =c("red", "grey85", "blue", "grey45"))+
  theme(#axis.text = element_text(angle=45, hjust=1, size=12),
        strip.text = element_text(size=10, face="bold"),
        title = element_text(size=10, face="bold"),
        axis.text = element_text(size=10, angle=45, hjust=1),
        legend.title = element_blank(),
        legend.text=element_text(size=10),
        legend.position = "bottom",
        legend.justification = "center")+labs(fill="Trend")+
  guides(fill=guide_legend(nrow=1, legend.justification="center"))+
  facet_wrap(~monthWord, nrow=2)
setwd(images.wd)
ggsave("trendIslandMap.png", device="png",  width=5.5, height=8, units="in")






island.trends %>% as_tibble() %>% select(-geometry, -area) %>% left_join(island.nest %>% select(month, fid, data), by=c("fid", "month")) %>% unnest(data) %>% 
  group_by(fid, month, year, monthWord) %>% summarise(mean.class = mean(class.mean)) %>% ungroup() %>% 
  left_join(island.trends %>% as_tibble() %>% select(-geometry, -area, -monthWord), by=c("fid", "month") ) %>% 
  ggplot(aes(x=year, y=mean.class, color=trend, group=fid))+
    geom_point()+
    geom_line()+facet_grid(vars(monthWord), vars(trend))


```   




# OLD



```{r}
## Create the gif ----- needs to be updated for single image classification
setwd(gif.wd)
prep.gif.data = all.classified.filter %>% filter(month==6) %>% 
  st_as_sf() %>% st_transform(crs = crs.plot) %>% 
  group_by(year) %>% nest()
### Loop through each year
for (z in 1: length(prep.gif.data$year)){
  dat = prep.gif.data$data[[z]]
  year.main = prep.gif.data$year[[z]]
  
  #### Create plot 1 (map of June connectivity)
  p1 = ggplot(data=dat)+
   geom_sf(aes(fill=.pred_class), color=NA)+
    scale_fill_manual(values=c("#619CFF","#00BA38","#F8766D"))+theme_bw()+
    annotation_scale(text_cex = 0.9)+
    geom_sf(data=mack.basin.large, color="grey65")+
    geom_sf(data=study.area, color=NA, fill=NA)+
    geom_sf(data=combo.location, aes(color=rivr), size=5)+
    scale_colour_manual(guide="none",     values=c("#000000", "#ABA9A9"))+
    theme(axis.text.x = element_text(angle=45, hjust=1, size=12),
          strip.text = element_text(size=14, face="bold"),
          axis.text.y = element_text(size=12),
         # legend.title = element_text(size=16, face="bold"),
          legend.text=element_text(size=16, face="bold"),
          title=element_text(size=18, face="bold"),
          legend.position="bottom", legend.direction="horizontal",
          legend.key.size=unit(1, "cm"),
          legend.title=element_blank(),
         legend.box.spacing = unit(0, "pt"),
         legend.margin=margin(0,0,0,0))+labs(fill="Class")+
    guides(fill=guide_legend(label.position="top",label.vjust = -8, title.vjust = 0.2))
  
  #### create plot 2 (date of peak water level)
  p2= combo.df %>% as_tibble() %>% ggplot()+geom_line(aes(x=yr, y=max.first.doy, color=rivr))+
    geom_point(aes(x=yr, y=max.first.doy,color=rivr), size=1.5)+theme_bw()+
    geom_point(data=combo.df %>% filter(year==year.main), aes(x=year, y=max.first.doy, color=rivr), size=5)+
    ylab(str_wrap("date of peak water level", width=10))+
    xlab("")+
    #geom_smooth(method=lm, aes(x=yr, y=max.first.value), se=F)+
    theme(axis.text.x = element_text( size=12),
          #strip.text = element_text(size=14, face="bold"),
          title = element_text(size=14, face="bold"),
          axis.text.y = element_text(angle=45, hjust=1, vjust=-0.05,size=12),
          legend.title = element_blank(),
          legend.text=element_text(size=14),
          legend.position = "bottom")+
    scale_colour_manual(labels = function(x) str_wrap(x, width = 30),
                         values=c("#000000", "#ABA9A9") )+
    scale_y_continuous(breaks = c(121,127,135, 144,152, 160), labels = c("May 1st", "May 7th","May 15th",
                                                                         "May 24th", 
                                                                     "June 1st", "June 9th"))+
    labs(colour="WSC Station Name")+xlim(2000,2020)
  #### Create plot 3 (height of peak water level)
  p3=combo.df %>% as_tibble() %>% ggplot()+geom_line(aes(x=yr, y=max.first.value, color=rivr))+
    geom_point(aes(x=yr, y=max.first.value,color=rivr), size=1.5)+theme_bw()+
    geom_point(data=combo.df %>% filter(year==year.main), aes(x=year, y=max.first.value, color=rivr), size=5)+
    xlab("year")+
    ylab(str_wrap("peak water level (m)", width=10))+
    #geom_smooth(method=lm, aes(x=yr, y=max.first.value), se=F)+
    theme(axis.text.x = element_text( size=12),
          #strip.text = element_text(size=14, face="bold"),
          title = element_text(size=14, face="bold"),
          axis.text.y = element_text(size=12),
          legend.title = element_blank(),
          legend.text=element_text(size=14),
          legend.position="bottom")+
    scale_colour_manual(labels = function(x) str_wrap(x, width = 30),
                         values=c("#000000", "#ABA9A9") )+
    labs(colour="WSC Station Name")+xlim(2000,2020)
  #### Combine plots 2 and 3 into one plot 
  p23 = p2/p3+plot_layout(guides="collect")
  #### Combine the p2/3 plot with the map
  gif.plot = p1+p23+plot_layout(widths=c(4, 4)) & 
    theme(legend.position="bottom", plot.title = element_text(size=30,face="bold", hjust=0.5)) &
    plot_annotation(title=paste("June ",year.main))
  #### Save the gif to your file
  ggsave(plot=gif.plot,filename= paste0("year", year.main, ".png") ,width=13.8, height=9.5, units = "in") 
}

```















######################### OLD STUFF #########################


















# old
```{r}
# Import GECI validation data for training/testing
setwd(valFile.path)
lake.class = st_read(valFileName , stringsAsFactors = F) %>% as_tibble() %>% 
  rename(OBJECTID=ID)
lake.class %>% group_by(type) %>% count()
# group classes by functional connectivity
lake.class.grouped=lake.class %>% mutate(final_class = case_when(
  type == "g1" | type== "g7" | type== "g4" ~ "1", #always low functional
  type== "g3_5" ~ "2", #some at high dis, none at low dis
  type == "g3"  | type =="g6" ~ "3", #high at high dis, none at low dis
  type == "g2_5" ~ "4", #high at high dis, some at low dis
  type == "g2" | type=="g5" |type=="g2_add"~ "5", # high at both low and high dis
  type== "badImage" | type == "coastal" | type =="notLake" | type=="uncertain" ~ "remove",
  type=="mediumThenHigh" | type=="moderateBoth" | type=="lowThenMedium"~ "not enough data")) %>% select(-geometry, -type)
lake.class.grouped %>% group_by(final_class) %>% count()

# Now, use Boruta, see below, to figure out which variables are important
#https://www.datacamp.com/community/tutorials/feature-selection-R-boruta
set.seed(90)
trainData <- prep %>% left_join(lake.class.grouped, by="OBJECTID") %>% 
  dplyr::filter(!is.na(final_class) & (final_class != "remove") & (final_class !="not enough data")) %>% 
  mutate(classes=as.factor(final_class)) %>% dplyr::select(-final_class) %>%  
  dplyr::select(-OBJECTID) %>% drop_na()
## remove unnecessary columns using Boruta
attach(trainData)
colnames(trainData)
## apply the boruta
boruta.train <- Boruta(classes~., data = trainData,
                       mcAdj = TRUE, pValue = 0.01,doTrace = 2,ntree = 50)
# print the results of the boruta
print(boruta.train)
boruta.train$finalDecision
# plot the results of the boruta
plot(boruta.train, colCode=c("lightgreen", "yellow", "red", "blue"),
     cex.axis=.7, las=2, xlab="",sort=TRUE, main="") 
## Get rid of tentative (yellow on the plot)
boruta.bank=TentativeRoughFix(boruta.train)
plot(boruta.bank, colCode=c("lightgreen", "yellow", "red", "blue"),
     cex.axis=.7, las=2, xlab="",sort=TRUE, main="") 

#Select the variables to use during the classification
important.cols=getSelectedAttributes(boruta.bank, withTentative = F) 
bank_df <- attStats(boruta.bank)
print(bank_df) #visualize variable importance


###Create a decision tree! Tutorial here: https://www.gmudatamining.com/lesson-13-r-tutorial.html
set.seed(123)
geci.val =  prep %>% left_join(lake.class.grouped, by="OBJECTID") %>% 
  dplyr::filter(!is.na(final_class) & (final_class != "remove") & (final_class !="not enough data")) %>% 
  mutate(classes=as.factor(final_class)) %>% 
  dplyr::select(-final_class) %>%  
  dplyr::select(c("OBJECTID","classes", all_of(important.cols))) %>% drop_na()


#look at the data to make sure it is as expected
skimr::skim(geci.val)
# split data into test and train
set.seed(234)
geci.split = initial_split(geci.val, strata=classes)
geci.train = training(geci.split)
geci.test = testing(geci.split)

# Prepare data for later 5-fold cross validation
geci.folds5 = vfold_cv(geci.train, v=5, strata = classes)
# get the recipe setup for pre-processing
geci.recipe = recipe(classes~., data=geci.train) %>% 
   update_role(OBJECTID, new_role = "OBJECTID")  ###########################Maybe exclude the ID instead of updating the role, then adding it later
geci.recipe %>% prep() %>% bake(new_data=geci.train)#print the training data
# Set up the model and what parameters we want to tune
tree_model = decision_tree(cost_complexity=tune(),
                           tree_depth=tune(),
                           min_n=tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")
# setup the workflow with both the model and the preprocessing recipe
tree_workflow=workflow() %>% 
  add_model(tree_model) %>% 
  add_recipe(geci.recipe)
# Hyperparameter tuning
set.seed(345)
##Build a grid of parameter options
tree_grid = grid_regular(cost_complexity(), 
                         tree_depth(), 
                         min_n(), levels=5)
#tune the grid
tree_tuning = tree_workflow %>% 
  tune_grid(resamples = geci.folds5, grid = tree_grid)
## Show the top 5 best models based on roc_auc metric
tree_tuning %>% collect_metrics() %>% 
  ggplot()+geom_point(aes(x=min_n, y=mean, color=as.factor(tree_depth), size=cost_complexity), alpha=0.3)+facet_wrap(~.metric)
##select the best model based on accuracy (it is the same model whether we pick using accuracy or roc_auc)
best_tree = tree_tuning %>% 
  select_best(metric="accuracy")
best_tree

# Finalize workflow with the new tuned parameters
final_tree_workflow = tree_workflow %>% finalize_workflow(best_tree)
# Fit the model to the training dataset
set.seed(456)
tree_wf_fit = final_tree_workflow %>% fit(data=geci.train)
tree_fit = tree_wf_fit %>% extract_fit_parsnip()#show the  model
# expore the model
library(vip)
library(rpart.plot)
vip(tree_fit) #variable importance
tree.plot =rpart.plot::rpart.plot(tree_fit$fit, roundint=FALSE, type=5, extra=2)
tree.plot
# setwd(figures.filePath)
# ggsave(tree.plot, width=6.5, height=3.14, filename=tree.figure.name)
# apply the model to the test set
tree_last_fit = final_tree_workflow %>% last_fit(geci.split)
tree_last_fit %>% collect_metrics()
tree_predictions=tree_last_fit %>% collect_predictions()
conf_mat(tree_predictions, truth = classes, estimate = .pred_class)
confusionMatrix(tree_predictions$.pred_class, tree_predictions$classes)

```

## same thing but for random forrest
```{r}
# pre-process
geci.train= geci.train %>% drop_na()
geci.test = geci.test %>% drop_na()
geci.rec =recipe(classes ~., data=geci.train) %>% 
  update_role(OBJECTID, new_role = "OBJECTID") 
geci.pre=prep(geci.rec)
geci.juiced = juice(geci.pre)
geci.juiced %>% count(classes)
#Make model specifications & get ready to tune
tune.spec = rand_forest(
  mtry=tune(), #when you are making leaves of the tree, how many do you sample at each split--all predictors or just a few
  trees = 500,
  min_n=tune()# How long do you keep splitting. How many datapoints have to be in a node before you stop splitting
  ) %>% 
  set_mode("classification") %>% set_engine("ranger") #ranger is just one way of doing random forest
tune.wf=workflow() %>% 
  add_recipe(geci.rec) %>% 
  add_model(tune.spec)
# Train hyperparameters with 5-fold cross validation
set.seed(345)
geci.fold  = vfold_cv(geci.train, v=5, strata=classes)
## tune parameters
#doParallel::registerDoParallel()
tune.res=tune_grid(
  tune.wf,
  resamples=geci.fold,
  grid=20
)
## take a look at parameters
tune.res %>% collect_metrics() #look at all the metrics
tune.res %>% select_best("roc_auc") #select best accuracy
tune.res %>% 
  collect_metrics() %>% 
  filter(.metric=="roc_auc") %>% 
  select(mean, min_n, mtry) %>% 
  pivot_longer(min_n:mtry,
               values_to="value",
               names_to="parameter") %>% 
  ggplot(aes(value, mean, color=parameter))+
  geom_point(show.legend="FALSE")+
  facet_wrap(~parameter, scales="free_x")
# Tune again using info from prior tuning
set.seed(456)
rf.grid= grid_regular(
  mtry(range=c(5,20)),
  min_n(range=c(0,10)),
  levels = 5
)
regular.res=tune_grid(
  tune.wf,
  resamples=geci.fold,
  grid=rf.grid
)
##roc_auc
regular.res %>% 
  collect_metrics() %>% 
  filter(.metric=="roc_auc") %>% 
  mutate(min_n=factor(min_n)) %>% 
  ggplot(aes(mtry, mean, color=min_n))+geom_point()+geom_line(alpha=0.5, size=1.5)
##accuracy
regular.res %>% 
  collect_metrics() %>% 
  filter(.metric=="accuracy") %>% 
  mutate(min_n=factor(min_n)) %>% 
  ggplot(aes(mtry, mean, color=min_n))+geom_point()+geom_line(alpha=0.5, size=1.5)
# select the best option
best.acc =select_best(regular.res, "roc_auc")
final.rf=finalize_model(
  tune.spec,
  best.acc
)
# Check out variable importance for the model as a whole
library(vip)
final.rf %>% set_engine("ranger", importance="permutation") %>% 
  fit(classes~.,
      data = juice(geci.pre) %>% select(-OBJECTID)) %>% 
  vip(geom="point")
# see how the model does on the testing data
final.wf = workflow() %>% 
  add_recipe(geci.rec) %>% 
  add_model(final.rf)
final.res=final.wf %>% last_fit(geci.split)
final.res %>% collect_metrics()
final.res %>% collect_predictions() %>% 
  mutate(correct=case_when(classes==.pred_class~"Correct", TRUE ~"Incorrect")) %>% 
  bind_cols(geci.test) %>% 
  ggplot(aes(x=med_Nsmi_ratio_m,y=med_Gb_ratio_m, color=correct))+
  geom_point(size=3, alpha=0.4)+labs(color=NULL)+scale_color_manual(values=c("gray80", "darkred"))+theme_bw()
final.pred.rf= final.res %>% collect_predictions()

setwd("C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/intermediaryDownloads")
write_rds(final.pred.rf, "predictions_traintest_20220826.Rdata")


conf_mat(final.pred.rf, truth = classes, estimate = .pred_class)
table=confusionMatrix(final.pred.rf$.pred_class, final.pred.rf$classes)

table

## plot the training/testing dataset on a map
### This is the fitted model to use on the other datasets
fitted.wf.rf= pluck(final.res, 6)[[1]]
train.ids = geci.train$OBJECTID
test.ids = geci.test$OBJECTID
rf.class = cbind(predict(fitted.wf.rf, geci.val), geci.val) %>% as_tibble() %>% select(OBJECTID, .pred_class, classes)%>% rename(.obs_class = classes) %>% 
  mutate(split=case_when(
    OBJECTID %in% train.ids~"train", 
    OBJECTID %in% test.ids~"test")) %>% 
  gather(key="class",value="group", -OBJECTID, -split ) %>% 
  left_join(lakes.sf %>% 
              select(-Shape_Leng, -Shape_Area, -count), 
            by="OBJECTID") %>% 
  st_as_sf()

ggplot(rf.class)+geom_bar(aes(x=group, fill=class), 
                          stat="count", position="dodge") +
  facet_wrap(~split)+theme_bw()


rf.class.raw = cbind(predict(fitted.wf.rf, geci.val), geci.val) %>% as_tibble() %>% select(OBJECTID, .pred_class, classes)%>% rename(.obs_class = classes) %>% 
  mutate(split=case_when(
    OBJECTID %in% train.ids~"train", 
    OBJECTID %in% test.ids~"test")) %>% 
  left_join(lakes.sf %>% 
              select(-Shape_Leng, -Shape_Area, -count), 
            by="OBJECTID") %>% 
  st_as_sf()


rf.class.raw %>% filter(split=="test") %>% filter(.pred_class=="4") %>% mapview(zcol=".obs_class")



library(ggalluvial)
alluvial.prep=rf.class.raw %>% filter(split=="test") %>% as_tibble() %>% select(-geometry)%>% group_by(.pred_class, .obs_class) %>% summarise(freq=n())

ggplot(data=alluvial.prep, aes(axis1=.obs_class, axis2 = .pred_class, y=freq))+
  geom_alluvium(aes(fill=.pred_class))+geom_stratum(aes(fill=.pred_class))+
  geom_text(stat = "stratum",
            aes(label = after_stat(stratum))) +
  scale_x_discrete(limits = c("Survey", "Response"),
                   expand = c(0.15, 0.05)) +
  theme_void()




# apply classification to all lakes 
all.data= channels.lakes %>% as_tibble() %>% #filter(OBJECTID %in% goodLakes.val.ids) %>% 
  #filter(year %in%  highest.maxDis.yrs) %>% 
  # mutate(year.group = case_when(
  #   year>=2000 & year <=2001 ~ "2000-2001",
  #   year>=2002 & year <=2003 ~ "2002-2003",
  #   year>=2004 & year <=2005 ~ "2004-2005",
  #   year>=2006 & year <=2007 ~ "2006-2007",
  #   year>=2008 & year <=2009 ~ "2008-2009",
  #   year>=2010 & year <=2011 ~ "2010-2011",
  #   year>=2012 & year <=2013 ~ "2012-2013",
  #   year>=2014 & year <=2015 ~ "2014-2015",
  #   year>=2016 & year <=2017 ~ "2016-2017",
  #   year>=2018 & year <=2019 ~ "2018-2019",
  #   year>=2020 ~ "2020",
  # )) %>% 
  mutate(year.group = case_when(
    year>=2000 & year<=2003 ~ "2000-2003",
    year>=2004 & year<=2007 ~ "2004-2007",
    year>=2008 & year<=2011 ~ "2008-2011",
    year>=2012 & year<=2015 ~ "2012-2015",
    year>=2016 & year<=2019 ~ "2016-2019",
    year>=2020 & year<=2023 ~ "2020-2013",
  )) %>% 
  dplyr::group_by(OBJECTID, year.group) %>% 
  dplyr::summarise(med_dw_rat_m = median(dom_wv_ratio_m),
            sdev_dw_rat_m = sd(dom_wv_ratio_m),
            kurt_dw_rat_m = kurtosis(dom_wv_ratio_m),
           med_R_ratio_m = median(R_ratio_m),
           sdev_R_ratio_m = sd(R_ratio_m),
           kurt_R_ratio_m = kurtosis(R_ratio_m),
           med_B_ratio_m = median(R_ratio_m),
           sdev_B_ratio_m = sd(R_ratio_m),
           kurt_B_ratio_m = kurtosis(R_ratio_m),
           med_G_ratio_m = median(R_ratio_m),
           sdev_G_ratio_m = sd(R_ratio_m),
           kurt_G_ratio_m = kurtosis(R_ratio_m),
           med_Gb_ratio_m = median(Gb_ratio_m),
           sdev_Gb_ratio_m = sd(Gb_ratio_m),
           kurt_Gb_ratio_m = kurtosis(Gb_ratio_m),
           med_Ndssi_ratio_m = median(Ndssi_ratio_m),
           sdev_Ndssi_ratio_m = sd(Ndssi_ratio_m),
           kurt_Ndssi_ratio_m = kurtosis(Ndssi_ratio_m),
           med_Nsmi_ratio_m = median(Nsmi_ratio_m),
           sdev_Nsmi_ratio_m = sd(Nsmi_ratio_m),
           kurt_Nsmi_ratio_m = kurtosis(Nsmi_ratio_m),
           count=n()) %>% ungroup() %>% filter(count>=10) %>% select(-count)



# see how the model does on the testing data
mod=final.wf %>% fit(rbind(geci.train, geci.test))
all.pred = predict(mod, all.data)

all.res = cbind.data.frame(all.data, all.pred) %>% select(OBJECTID, year.group, .pred_class) %>% 
  as_tibble() %>% left_join(lakes.sf, by="OBJECTID") %>% st_as_sf()

ggplot(data=all.res, aes(fill=.pred_class))+geom_sf(color=NA)+facet_wrap(~year.group)

```


```{r}
setwd(valFile.path)
lake.structure = st_read(valFileName , stringsAsFactors = F) %>% as_tibble() %>% 
  rename(OBJECTID=ID) %>% 
  mutate(
    structure.class = case_when(
      type %in% c("g5", "g6", "g7") ~ "no channel",
      type %in% c("g2","g2_5", "g3", "g3_5", "g4", 
                  "moderateBoth", "lowThenMedium", "mediumThenHigh", "g2_add") ~
        "channel"
    )
  ) %>% 
  dplyr::filter(!is.na(structure.class) ) %>% 
  dplyr::select(-type)





```




```{r}
#Classify using the other method...
prep %>% ggplot()+geom_density(aes(x=med_Nsmi_ratio_m))
classifications=prep %>% 
  mutate(connectivity=ifelse(med_Nsmi_ratio_m>=0.98 &  
                                 sdev_Gb_ratio_m<0.24, 
                             "high functional connectivity", "low functional connectivity"))


# do the "through time" classification
connectivity.groups = classifications%>% dplyr::select(ID, time_period, connectivity) %>% 
  spread(time_period, connectivity)


# get lakes that are always high functional connectivity
always.high.ids=connectivity.groups %>% 
  filter(`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="high functional connectivity")
always.high.ids=always.high.ids$ID

#get lakes that are always low functional connectivity
always.low.ids = connectivity.groups %>% 
  filter(`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="low functional connectivity")
always.low.ids=always.low.ids$ID

# get lakes that go from high to low connectivity over time
high.to.low.ids = connectivity.groups %>% 
  filter((`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="low functional connectivity") |
           (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="low functional connectivity")|
          (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="low functional connectivity")
           )
high.to.low.ids=high.to.low.ids$ID

# get lakes that go from low to high connectivity over time
low.to.high.ids = connectivity.groups %>% 
  filter((`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="high functional connectivity") |
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="high functional connectivity")|
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="high functional connectivity"))
low.to.high.ids=low.to.high.ids$ID

# get lakes that flip back and forth connectivity through time
flip.ids = connectivity.groups %>% 
  filter((`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="high functional connectivity") |
           (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="high functional connectivity")|
           (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="low functional connectivity")|
          (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="high functional connectivity")|
         (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="low functional connectivity")|
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="low functional connectivity")|
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="high functional connectivity")|
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="low functional connectivity")
         )
flip.ids = flip.ids$ID

# classify connectivity variability through time
final.lakes=connectivity.groups %>% mutate(
  group = case_when(
    ID %in% always.high.ids ~ "always high functional connectivity",
    ID %in% always.low.ids ~ "always low functional connectivity",
    ID %in% high.to.low.ids ~ "high to low functional connectivity over time",
    ID %in% low.to.high.ids ~ "low to high functional connectivity over time",
    ID %in% flip.ids ~ "connectivity switches back and forth through time"
  )
) %>% filter(!is.na(group)) %>%
  mutate(group2 = ifelse(ID %in%flip.ids| 
                           ID %in% low.to.high.ids |
                           ID %in% high.to.low.ids, 
                         "variable functional connectivity", group))  

# import shapefile
setwd("C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/shapeFiles")
mack.lakes.sf = st_read("mackenzieGoodLakes.shp") %>% rename(ID=OBJECTID)
set.seed(100)
sample.mack.lakes = mack.lakes.sf %>% sample_n(length(mack.lakes.sf$ID)*0.2) %>% 
  mutate(fxd_ndx=row_number())
st_write(sample.mack.lakes, "mackenzieLakes20pct.shp")

#join shapefile to results
results.sf = final.lakes %>% left_join(mack.lakes.sf, by="ID") %>% st_as_sf()
mapview(results.sf, zcol="group")
```

